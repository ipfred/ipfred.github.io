[{"categories":null,"content":" 1 网站 小林coding (xiaolincoding.com)🥓 🍳刘沙河 (bigox.wiki) 🧇爱编程的大丙 (subingwen.cn) ","date":"2024-04-25","objectID":"/web/:1:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":" 2 常用官方API MySQL官方文档🍕 Hive官方文档🍔 官方API- Spark 2.4.8🍟 ","date":"2024-04-25","objectID":"/web/:2:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":" 3 工具类 在线Cron表达式生成器 (qqe2.com) RegExr: Learn, Build, \u0026 Test RegEx The-X 在线工具箱 Base64 解码 AES RAS 解码 加密 ","date":"2024-04-25","objectID":"/web/:3:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":" 4 影视 BTNULL 无名小站🍿 ","date":"2024-04-25","objectID":"/web/:4:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":" 5 图书 安娜的档案 (annas-archive.org)🧂 ","date":"2024-04-25","objectID":"/web/:5:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"Hive官方操作手册 ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:0:0","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" 1 Hive特性 hive基于Hadoop，将结构化数据映射为一张数据库表，提供类SQL查询功能 Hive元数据存储,通常存储在关系形数据库中，Hive中的元数据包括(表的名称，列，分区，是否为外部表等属性，数据所在的路径) Hive支持MapReduce、Tez和Spark 三种计算引擎。 数据格式。数据格式用户定义，根据三个属性：列分隔符（通常为空格、”\\t”、”\\x001″）、行分隔符（”\\n”）以及读取文件数据的方法（Hive 中默认有三个文件格式 TextFile，SequenceFile 以及 RCFile） 数据更新。不支持对数据的更新，使用更新可以借助Impala引擎+kudu数据格式 索引。有hive版本支持建立索引，但是各种问题，不建议使用。 Hive中包含的数据模型，DB、Table，External Table，Partition，Bucket db：在hdfs中表现为hive.metastore.warehouse.dir目录下一个文件夹。 table：在hdfs中表现所属db目录下一个文件夹。 external table：与table类似，不过其数据存放位置可以在任意指定路径。 partition：在hdfs中表现为table目录下的子目录。 bucket：在hdfs中表现为同一个表目录下根据hash散列之后的多个文件。 ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:1:0","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" 2 Hive表类型","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:0","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" 2.1 hive数据类型 Hive的基本数据类型有：TINYINT，SAMLLINT，INT，BIGINT，BOOLEAN，FLOAT，DOUBLE，STRING，TIMESTAMP(V0.8.0+)和BINARY(V0.8.0+)。 Hive的集合类型有：STRUCT，MAP和ARRAY。 Hive表：内部表、外部表、分区表和桶表。 表的元数据保存传统的数据库的表中，当前hive只支持Derby和MySQL数据库。 ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:1","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" 2.2 Hive内部表Hive中的内部表和传统数据库中的表在概念上是类似的，Hive的每个表都有自己的存储目录，除了外部表外，所有的表数据都存放在配置在hive-site.xml文件的${hive.metastore.warehouse.dir}/table_name目录下。 -- 创建内部表 CREATE TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, grade STRING COMMOT '班级'）COMMONT '学生表' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORE AS TEXTFILE; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:2","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" 2.3 Hive外部表被external修饰的为外部表（external table），外部表指向已经存在在Hadoop HDFS上的数据，除了在删除外部表时只删除元数据而不会删除表数据外，其他和内部表很像 -- 创建外部表 CREATE EXTERNAL TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, class STRING COMMOT '班级'）COMMONT '学生表' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORE AS SEQUENCEFILE LOCATION '/usr/test/data/students.txt'; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:3","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" 2.4 Hive分区表分区表的每一个分区都对应数据库中相应分区列的一个索引，分区表中的每一个分区对应一个目录文件，即同一个分区的数据存放一个目录下面，所以分区表如果很大，指定分区会速度快很多 比如说，分区表partitinTable有包含nation(国家)、ds(日期)和city(城市)3个分区，其中nation = china，ds = 20130506，city = Shanghai则对应HDFS上的目录为： /datawarehouse/partitinTable/nation=china/city=Shanghai/ds=20130506/。 -- 创建分区表 CREATE TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, class STRING COMMOT '班级'）COMMONT '学生表' PARTITIONED BY (ds STRING,country STRING) -- 分区的列不属于建表的字段中 ROW FORMAT DELIMITED --分隔符 FIELDS TERMINATED BY ',' -- 结束换行符 STORE AS SEQUENCEFILE -- 数据存储格式 ; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:4","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" 2.5 Hive分桶表对指定列进行HASH，根据hash值进行切分数据，不同hash结果的数据写到每个桶对应的文件目录中 -- 创建分桶表 CREATE TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, class STRING COMMOT '班级',score SMALLINT COMMOT '总分'）COMMONT '学生表' PARTITIONED BY (ds STRING,country STRING) CLUSTERED BY(user_no) SORTED BY(score) INTO 32 BUCKETS -- 分桶 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORE AS SEQUENCEFILE; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:5","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" 2.6 Hive视图逻辑数据结构，将查询结果作为视图，简化查询操作 CREATE VIEW employee_skills AS SELECT name, skills_score['DB'] AS DB, skills_score['Perl'] AS Perl, skills_score['Python'] AS Python, skills_score['Sales'] as Sales, skills_score['HR'] as HR FROM employee; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:6","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" 2.7 总结表的创建官方标准 LanguageManual DDL -创建表的语法结构 CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables) -- 用另外一个表的结构来创建 CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; 删除表 DROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later) 清空表 TRUNCATE [TABLE] table_name [PARTITION partition_spec]; partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...) ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:7","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" HDFS ","date":"2024-04-24","objectID":"/bigdata/hadoop/a9bc224/:0:0","tags":["Hadoop"],"title":"HDFS文件读写过程","uri":"/bigdata/hadoop/a9bc224/"},{"categories":null,"content":" 1 文件的写入过程HDFS把文件的数据划分成若干个块(Block)， 每个Block存放在一组 DataNode上，Namenode负责维护文件–\u003eBlock(命令空间映射)和Block–\u003eDatanode(数据块映射) 写入流程 Client发起文件上传请求，通过RPC与NameNode建立通讯，NameNode检查目标文件是否已存在，父目录是否存在，返回是否可以上传 Client请求确定第一个block 应该传输到哪些Datanode服务器上 NameNode根据配置文件指定的备份数量以及机架感知原理进行文件分配，返回可用的 DataNode地址，如：A，B，C 备份数量：默认在HDFS上存放三份，本地一份、同机架内其他节点一份、不同机架节点一份 Client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline ），A 收到请求会继续调用 B，然后 B 调用 C，将整个 pipeline 建立完成， 后逐级返回 client； Client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存），以 packet 为单位（默认64K），A 收到一个 packet 就会传给 B，B 传给 C。A 每传一个 packet 会放入一个应答队列等待应答； 数据被分割成一个个 packet 数据包在 pipeline 上依次传输，在 pipeline 反方向上， 逐个发送 ack（命令正确应答），最终由 pipeline 中第一个 DataNode 节点 A 将 pipelineack 发送给 Client； 当一个 block 传输完成之后，Client 再次请求 NameNode 上传第二个 block，重复步骤 2； ","date":"2024-04-24","objectID":"/bigdata/hadoop/a9bc224/:1:0","tags":["Hadoop"],"title":"HDFS文件读写过程","uri":"/bigdata/hadoop/a9bc224/"},{"categories":null,"content":" 2 HDFS文件读取过程 Client远程调用请求NameNode，获取文件块位置列表 NameNode会视情况返回文件的部分或者全部block列表；对于每个block，NameNode返回含副本的所有DataNode 地址；计算传输最快最优的DataNode 返回的DataNode地址，会按照集群拓扑结构计算客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 STALE，这样的排靠后； Client 选取排序靠前的 DataNode建立输入流，如果客户端本身就是DataNode，那么将从本地直接获取数据(短路读取特性)； 在选定的DataNode上读取该Block的数据 当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表； 读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的DataNode 继续读。 read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回Client请求包含块的DataNode地址，并不是返回请求块的数据； 最终读取来所有的 block 会合并成一个完整的最终文件。 总结：串行写入，数据包先发给节点A，然后节点A发送给B，B在给C 并行读取，并行读取block所在的节点，最后合并 ","date":"2024-04-24","objectID":"/bigdata/hadoop/a9bc224/:2:0","tags":["Hadoop"],"title":"HDFS文件读写过程","uri":"/bigdata/hadoop/a9bc224/"},{"categories":["project"],"content":" 1 HDFS 的命令行使用如果没有配置 hadoop 的环境变量，则在 hadoop 的安装目录下的bin目录中执行以下命令，如已配置 hadoop 环境变量，则可在任意目录下执行 help 格式: hdfs dfs -help 操作命令 作用: 查看某一个操作命令的参数信息 ls 格式：hdfs dfs -ls URI 作用：类似于Linux的ls命令，显示文件列表 lsr 格式 : hdfs dfs -lsr URI 作用 : 在整个目录下递归执行ls, 与UNIX中的ls-R类似 mkdir 格式 ： hdfs dfs -mkdir [-p] \u003cpaths\u003e 作用 : 以\u003cpaths\u003e中的URI作为参数，创建目录。使用-p参数可以递归创建目录 put 格式 ： hdfs dfs -put \u003clocalsrc \u003e ... \u003cdst\u003e 作用 ： 将单个的源文件src或者多个源文件srcs从本地文件系统拷贝到目标文件系统中（\u003cdst\u003e对应的路径）。也可以从标准输入中读取输入，写入目标文件系统中 hdfs dfs -put /rooot/bigdata.txt /dir1 moveFromLocal 格式： hdfs dfs -moveFromLocal \u003clocalsrc\u003e \u003cdst\u003e 作用: 和put命令类似，但是源文件localsrc拷贝之后自身被删除 hdfs dfs -moveFromLocal /root/bigdata.txt / copyFromLocal 格式: hdfs dfs -copyFromLocal \u003clocalsrc\u003e ... \u003cdst\u003e 作用: 从本地文件系统中拷贝文件到hdfs路径去 appendToFile 格式: hdfs dfs -appendToFile \u003clocalsrc\u003e ... \u003cdst\u003e 作用: 追加一个或者多个文件到hdfs指定文件中.也可以从命令行读取输入. hdfs dfs -appendToFile a.xml b.xml /big.xml moveToLocal 在 hadoop 2.6.4 版本测试还未未实现此方法 格式：hadoop dfs -moveToLocal [-crc] \u003csrc\u003e \u003cdst\u003e 作用：将本地文件剪切到 HDFS get 格式 hdfs dfs -get [-ignorecrc ] [-crc] \u003csrc\u003e \u003clocaldst\u003e 作用：将文件拷贝到本地文件系统。 CRC 校验失败的文件通过-ignorecrc选项拷贝。 文件和CRC校验可以通过-CRC选项拷贝 hdfs dfs -get /bigdata.txt /export/servers getmerge 格式: hdfs dfs -getmerge \u003csrc\u003e \u003clocaldst\u003e 作用: 合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,... copyToLocal 格式: hdfs dfs -copyToLocal \u003csrc\u003e ... \u003clocaldst\u003e 作用: 从hdfs拷贝到本地 mv 格式 ： hdfs dfs -mv URI \u003cdest\u003e 作用： 将hdfs上的文件从原路径移动到目标路径（移动之后文件删除），该命令不能跨文件系统 hdfs dfs -mv /dir1/bigdata.txt /dir2 rm 格式： hdfs dfs -rm [-r] 【-skipTrash】 URI 【URI 。。。】 作用： 删除参数指定的文件，参数可以有多个。 此命令只删除文件和非空目录。 如果指定-skipTrash选项，那么在回收站可用的情况下，该选项将跳过回收站而直接删除文件； 否则，在回收站可用时，在HDFS Shell 中执行此命令，会将文件暂时放到回收站中。 hdfs dfs -rm -r /dir1 cp 格式: hdfs dfs -cp URI [URI ...] \u003cdest\u003e 作用： 将文件拷贝到目标路径中。如果\u003cdest\u003e 为目录的话，可以将多个文件拷贝到该目录下。 -f 选项将覆盖目标，如果它已经存在。 -p 选项将保留文件属性（时间戳、所有权、许可、ACL、XAttr）。 hdfs dfs -cp /dir1/a.txt /dir2/bigdata.txt cat hdfs dfs -cat URI [uri ...] 作用：将参数所指示的文件内容输出到stdout hdfs dfs -cat /bigdata.txt tail 格式: hdfs dfs -tail path 作用: 显示一个文件的末尾 text 格式:hdfs dfs -text path 作用: 以字符形式打印一个文件的内容 chmod 格式:hdfs dfs -chmod [-R] URI[URI ...] 作用：改变文件权限。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 hdfs dfs -chmod -R 777 /bigdata.txt chown 格式: hdfs dfs -chmod [-R] URI[URI ...] 作用： 改变文件的所属用户和用户组。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 hdfs dfs -chown -R hadoop:hadoop /bigdata.txt df 格式: hdfs dfs -df -h path 作用: 统计文件系统的可用空间信息 du 格式: hdfs dfs -du -s -h path 作用: 统计文件夹的大小信息 count 格式: hdfs dfs -count path 作用: 统计一个指定目录下的文件节点数量 setrep expunge (慎用) 格式: hdfs dfs -setrep num filePath 作用: 设置hdfs中文件的副本数量 注意: 即使设置的超过了datanode的数量,副本的数量也最多只能和datanode的数量是一致的 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:1:0","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":" 2 hdfs的高级使用命令","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:0","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":" 2.1 HDFS文件限额配置在多人共用HDFS的环境下，配置设置非常重要。特别是在 Hadoop 处理大量资料的环境，如果没有配额管理，很容易把所有的空间用完造成别人无法存取。HDFS 的配额设定是针对目录而不是针对账号，可以让每个账号仅操作某一个目录，然后对目录设置配置。 HDFS 文件的限额配置允许我们以文件个数，或者文件大小来限制我们在某个目录下上传的文件数量或者文件内容总量，以便达到我们类似百度网盘网盘等限制每个用户允许上传的最大的文件的量。 hdfs dfs -count -q -h /user/root/dir1 #查看配额信息 2.1.1 数量限额 hdfs dfs -mkdir -p /user/root/dir #创建hdfs文件夹 hdfs dfsadmin -setQuota 2 dir # 给该文件夹下面设置最多上传两个文件，发现只能上传一个文件 hdfs dfsadmin -clrQuota /user/root/dir # 清除文件数量限制 2.1.2 空间大小限额在设置空间配额时，设置的空间至少是 block_size * 3 大小 hdfs dfsadmin -setSpaceQuota 4k /user/root/dir # 限制空间大小4KB hdfs dfs -put /root/a.txt /user/root/dir # 生成任意大小的文件 dd if=/dev/zero of=1.txt bs=1M count=2 #生成2M的文件 # 清除空间配额限制 hdfs dfsadmin -clrSpaceQuota /user/root/dir ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:1","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":" 2.2 HDFS 的安全模式安全模式是hadoop的一种保护机制，用于保证集群中的数据块的安全性。当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。 假设我们设置的副本数（即参数dfs.replication）是3，那么在datanode上就应该有3个副本存在，假设只存在2个副本，那么比例就是2/3=0.666。hdfs默认的副本率0.999。我们的副本率0.666明显小于0.999，因此系统会自动的复制副本到其他dataNode，使得副本率不小于0.999。如果系统中有5个副本，超过我们设定的3个副本，那么系统也会删除多于的2个副本。 在安全模式状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在，当整个系统达到安全标准时，HDFS自动离开安全模式。30s 安全模式操作命令 hdfs dfsadmin -safemode get #查看安全模式状态 hdfs dfsadmin -safemode enter #进入安全模式 hdfs dfsadmin -safemode leave #离开安全模式 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:2","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["开发"],"content":" HDFS HDFS概述 Hadoop 分布式系统框架中，首要的基础功能就是文件系统，在 Hadoop 中使用 FileSystem 这个抽象类来表示我们的文件系统，这个抽象类下面有很多子实现类，究竟使用哪一种，需要看我们具体的实现类，在我们实际工作中，用到的最多的就是HDFS(分布式文件系统)以及LocalFileSystem(本地文件系统)了。 在现代的企业环境中，单机容量往往无法存储大量数据，需要跨机器存储。统一管理分布在集群上的文件系统称为分布式文件系统。 HDFS（Hadoop Distributed File System）是 Hadoop 项目的一个子项目。是 Hadoop 的核心组件之一， Hadoop 非常适于存储大型数据 (比如 TB 和 PB)，其就是使用 HDFS 作为存储系统. HDFS 使用多台计算机存储文件，并且提供统一的访问接口，像是访问一个普通文件系统一样使用分布式文件系统。 HDFS架构 HDFS是一个主/从（Mater/Slave）体系结构，由三部分组成： NameNode 和 DataNode 以及 SecondaryNamenode： ● NameNode 负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。 ● DataNode 负责管理用户的文件数据块，每一个数据块都可以在多个 DataNode 上存储多个副本，默认为3个。 ● Secondary NameNode 用来监控 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS 元数据的快照。最主要作用是辅助 NameNode 管理元数据信息。 HDFS的特性 首先，它是一个文件系统，用于存储文件，通过统一的命名空间目录树来定位文件； 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 master/slave 架构（主从架构） HDFS 采用 master/slave 架构。一般一个 HDFS 集群是有一个 Namenode 和一定数目的 Datanode 组成。Namenode 是 HDFS 集群主节点，Datanode 是 HDFS 集群从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。 分块存储(Block) HDFS 中的文件在物理上是分块存储（block）的，块的大小可以通过配置参数来规定，默认大小在 hadoop2.x 版本中是 128M。 名字空间（NameSpace） HDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。 Namenode 负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被 Namenode 记录下来。 HDFS 会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。 NameNode 元数据管理 我们把目录结构及文件分块位置信息叫做元数据。NameNode 负责维护整个 HDFS 文件系统的目录树结构，以及每一个文件所对应的 block 块信息（block 的 id，及所在的 DataNode 服务器）。 DataNode 数据存储 文件的各个 block 的具体存储管理由 DataNode 节点承担。每一个 block 都可以在多个 DataNode 上。DataNode 需要定时向 NameNode 汇报自己持有的 block 信息。 存储多个副本（副本数量也可以通过参数设置 dfs.replication，默认是 3） 副本机制 为了容错，文件的所有 block 都会有副本。每个文件的 block 大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。 一次写入，多次读出 HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的修改。 正因为如此，HDFS 适合用来做大数据分析的底层存储服务，并不适合用来做网盘等应用，因为修改不方便，延迟大，网络开销大，成本太高。 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E6%A6%82%E8%A7%88/:0:0","tags":["Hadoop"],"title":"Hadoop概览","uri":"/bigdata/hadoop/hadoop%E6%A6%82%E8%A7%88/"},{"categories":["开发"],"content":" 1 Apache Spark A Unified engine for large-scale data analytics Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing. ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:1:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":["开发"],"content":" 2 DownloadingGet Spark from the downloads page of the project website. This documentation is for Spark version 3.5.1. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath. Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI. If you’d like to build Spark from source, visit Building Spark. ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:2:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":null,"content":" IntroductionThis is bold text, and this is emphasized text. Visit the Hugo website! ","date":"2024-04-20","objectID":"/posts/my-first-post/:1:0","tags":null,"title":"My First Post","uri":"/posts/my-first-post/"},{"categories":null,"content":" 常用数据库的读写方式A blog (a truncation of “weblog”) is an informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (posts). Posts are typically displayed in reverse chronological order so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual,[citation needed] occasionally of a small group, and often covered a single subject or topic. In the 2010s, “multi-author blogs” (MABs) emerged, featuring the writing of multiple authors and sometimes professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other “microblogging” systems helps integrate MABs and single-author blogs into the news media. Blog can also be used as a verb, meaning to maintain or add content to a blog. ","date":"2024-02-20","objectID":"/posts/second_post/:1:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":null,"content":" 标题二","date":"2024-02-20","objectID":"/posts/second_post/:2:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":null,"content":" 标题三","date":"2024-02-20","objectID":"/posts/second_post/:3:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":["开发"],"content":" 1 Pythonpython第二篇文章扽扽 ","date":"2023-02-20","objectID":"/lang/python/first/:1:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":" 2 标题二","date":"2023-02-20","objectID":"/lang/python/first/:2:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":" 3 标题三","date":"2023-02-20","objectID":"/lang/python/first/:3:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":" 1 Pythonpython第一篇文章扽扽 ","date":"2023-02-20","objectID":"/lang/python/second/:1:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":" 2 标题二","date":"2023-02-20","objectID":"/lang/python/second/:2:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":" 3 标题三","date":"2023-02-20","objectID":"/lang/python/second/:3:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":" 常用数据库的读写方式A blog (a truncation of “weblog”) is an informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (posts). Posts are typically displayed in reverse chronological order so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual,[citation needed] occasionally of a small group, and often covered a single subject or topic. In the 2010s, “multi-author blogs” (MABs) emerged, featuring the writing of multiple authors and sometimes professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other “microblogging” systems helps integrate MABs and single-author blogs into the news media. Blog can also be used as a verb, meaning to maintain or add content to a blog. ","date":"2023-02-20","objectID":"/posts/first_post/:1:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"},{"categories":["开发"],"content":" 标题二","date":"2023-02-20","objectID":"/posts/first_post/:2:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"},{"categories":["开发"],"content":" 标题三","date":"2023-02-20","objectID":"/posts/first_post/:3:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"}]