[{"categories":null,"content":"\r2 Java核心基础 我想和自己说：学习是反复的过程，学习新知识的时候要仔仔细细阅读，错过一个关键词，可能对知识就会有偏差的理解。如果现在没有心情，收藏起来，后面用到的时候再仔细看。希望有帮助。 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:0:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.0 基本概念先了解","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r类 类是Java中的基本编程单元，用于描述对象的属性和行为。通过实例化类，可以创建对象 类是封装了数据和方法的结构 Java中，类概念非常核心和基础，用于组织和构建整个程序。 类名和文件名是一致的(后面讲class关键词的时候会细聊) ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:1","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r接口 接口是一种抽象的类型，它定义了一组方法的签名，但不提供方法的具体实现。 在后面写面向对象章节的时候，会对类和接口进行详解。 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:2","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r变量 在程序执行过程中，值可以在某个范围内发生改变的量。 变量要明确保存数据的数据类型 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:3","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r包 将相关的类和接口组织在一起，一个包下面可以创建很多类文件和接口文件。直接引用功能包，可能节省代码量。 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:4","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r常量 在程序执行过程中，值不发生改变的量。 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:5","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.1 注释什么是注释，就是对程序进行解释和说明的文字 单行注释 //单行注释 多行注释 /* 多行注释 */ 文档注释 /** * 这是一个文档注释示例 * 它通常包含有关类、方法或字段的详细信息 */ public class MyClass { // 类的成员和方法 } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:2:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.2 关键字被Java语言赋予了特殊含义的单词，在idea中会高亮显示，学习完基础知识后，可以再回过头 看看，对每个关键字的用法要熟悉，不然就是没入门 关键字官方文档 https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html 用于定义数据类型的关键字 class 类的标识 class HelloWorld {}, class是Java程序的基本构建块，它包含了数据和方法(死记就行)。Java文件名必须和类名保持一致，为了编译时能够正确识别和定位类(规定，死记就行); 如果一个文件中包含多个类，只能有一个类声明为 public，并且文件名必须与public类一致 public class MyClass { // 主类 文件名必须是 MyClass.java，程序入口点所在的类，必须要用public修饰，为了执行程序的时候，能够被jvm虚拟机访问到 public static void main(String[] args) { // 入口点 } } class AnotherClass {// 非公共类，在同一个文件中可以有多个} class YetAnotherClass {// 非公共类，建议每个类都存储在独立的文件中，提高代码可读性，如果有多个相关的类，放在同一个包内} interface 接口的标识 interface MyInterface {} enum 枚举 enum Day { SUNDAY, MONDAY, TUESDAY } byte、short、int、long、float、double、char、boolean(八大基本数据类型) void 声明方法没有返回值 流程控制关键字 if else switch case default while do for break continue return 包的关键字 package：将相关的类和接口组织在一起，一个包下面可以创建很多类文件和接口文件，在Java代码中 声明包 package com.example.myapp 包名通常是反转的域名，看到包就知道是什么功能 import 导包，引入其他包中的类 import com.otherpackage.OtherClass 访问权限修饰符关键字 访问控制权限在Java中为了管理类，方法，变量等成员在其他类中的可见性和访问权限。有利于控制代码的封装性，安全性和可维护性 private 私有的 成员只对声明它的类可见，其他类无法访问 protected 受保护访问 同一包内的类和所有子类可见 public: 成员对所有类可见，其他类可以自由访问 扩展：default（package-private）默认访问级别，没有修饰符，成员对统一包内的类可见 类，函数，变量修饰符关键字 abstract用于声明抽象类、抽象方法。抽象类不能被实例化，通常包含抽象方法，子类需要实现这些抽象方法。 什么是抽象类和抽象方法 final 用于声明不可被继承的类 final class FinalClass{};声明不可被重写的方法 final void finalMethod() {};声明不可被修改的变量 final int constantValue = 10; static表名具有静态属性。 // 静态字段属于类，所有类的实例共享相同的静态字段。 // 通过类名直接访问 class Myclass { static int staticField = 10; // 静态方法属于类，不用创建实例，通过类名直接访问 static void staticMethod() { } // 静态块 用于类加载时执行一些初始化操作 static {// 静态块} } synchronized 修饰方法或代码块，确保同一时刻只有同一个线程可以访问修饰的部分 类与类之间关系关键字 extends：用于类之间的继承关系 implements：接口的实现 实例相关关键字 new 创建对象实例 ClassName obj = new ClassName(); this：代表当前对象的引用，在类的方法中使用 this.variable指的是当前对象的成员变量 super：用于调用父类的方法或访问父类的成员 instanceof：用于检查对象是否是特定类的实例 if (obj instanceof ClassName) 异常处理关键字 try、catch、finally、throw、throws 其他修饰符关键字 native strictfp transient volatile assert：断言检查 assert age \u003e= 18 : \"必须年满18岁\"; ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:3:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.3 标识符标识符就是用来给类，接口，方法，变量，包等其名字的规则 类、接口 : 大驼峰命名法,第一个单词大写的是类\\接口 HelloWorld, VariableDemo 变量, 方法: 小驼峰命名法 第一个单词小写的是方法和变量 zhangSanAge, studentName 常量：所有字母都大写 包：所有字母全部都小写 com.baidu ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:4:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.4 数据类型Java是强类型语言，每一个数据都给出了明确的数据类型 基本数据类型(简称: 基本类型) byte, short, char, int, long, float, double, boolean 定义long类型的数据时, 数据后边要加字母L(大小写均可), 建议加L 定义float类型的数据时, 数据后边要加字母F(大小写均可), 建议加F 引用数据类型(简称: 引用类型) - String, 数组, 类, 接口 数据类型转换 不同类型的数据之间可能会进行运算，而这些数据取值范围不同，存储方式不同，直接进行运算可能会造成数据损失，所以需要类型转换 自动(隐式)类型转换 将取值范围小的类型自动提升为取值范围大的类型，byte、short、char--\u003eint--\u003elong--\u003efloat--\u003edouble 强制(显式)类型转换 将取值范围大的类型强制转换成取值范围小的类型. public static void main(String[] args) { double doubleValue = 123.456; int intValue = (int) doubleValue; // 强制将double转换为int System.out.println(\"原始double值：\" + doubleValue); System.out.println(\"强制转换后的int值：\" + intValue); /*原始double值：123.456 强制转换后的int值：123 */ } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:5:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.5 常量 整数常量 1 小数常量 3.14 字符常量 A B 20(20不是字符，是有两个字符组合成的) 字符串常量 \"abcd\" 布尔常量 true false 空常量 null ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:6:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.6 变量 在程序执行过程中，值可以在某个范围内发生改变的量叫变量 变量要有明确的数据类型 声明方式 数据类型 变量名=初始化的值; int a=10; 先声明，再赋值 int a; a=10; //1. 定义一个类, 类名叫: VariableDemo02 public class VariableDemo02 { //2. 定义main方法, 作为程序的主入口. public static void main(String[] args) { //3. 测试byte类型. //3.1 定义一个byte类型的变量, 变量名叫b, 初始化值为10. byte b = 10; //3.2 将变量b的值打印到控制台上. System.out.println(b); //4. 测试short类型. //4.1 定义一个short类型的变量, 变量名叫s, 初始化值为20. short s = 20; //4.2 将变量s的值打印到控制台上. System.out.println(s); //5. 测试char类型. //5.1 定义一个char类型的变量, 变量名叫c, 初始化值为'A'. char c = 'A'; //5.2 将变量c的值打印到控制台上. System.out.println(c); //6. 测试int类型 int a = 10; System.out.println(a); //7. 测试long类型, 数据后记得加字母L. long lon = 100L; System.out.println(lon); //8. 测试float类型, 数据后边加字母F. float f = 10.3F; System.out.println(f); //9. 测试double类型. double d = 5.21; System.out.println(d); //10. 测试boolean类型. boolean bb = true; System.out.println(bb); } } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:7:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.7 运算符","date":"2024-04-26","objectID":"/lang/java/20240426133928/:8:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.7.1分类 算术运算符 +, -, *, /, %, ++, – 变量前++ ：变量a自己加1，将加1后的结果赋值给b b=++a public static void main(String[] args) { int a = 1; int b = ++a; System.out.println(a);//计算结果是2 System.out.println(b);//计算结果是2 } 变量后++ ：变量a先把自己的值1，赋值给变量b public static void main(String[] args) { int a = 1; int b = a++; System.out.println(a);//计算结果是2 System.out.println(b);//计算结果是1 } 赋值运算符 =, +=, -=, *=, /=, %= 比较(关系)运算符 ==, !=, \u003e, \u003e=, \u003c, \u003c= 逻辑运算符 \u0026\u0026(并且), ||(或者), !(逻辑非), ^(逻辑异或) 异同的意思, 相同为false, 不同为true. public class LogicalOperatorsExample { public static void main(String[] args) { // 逻辑与运算符 (\u0026\u0026) boolean condition1 = true; boolean condition2 = false; // 如果两个条件都为true，则结果为true，否则为false boolean resultAnd = condition1 \u0026\u0026 condition2; System.out.println(\"逻辑与运算符 (\u0026\u0026) 结果：\" + resultAnd); // 逻辑或运算符 (||) // 如果至少一个条件为true，则结果为true，否则为false boolean resultOr = condition1 || condition2; System.out.println(\"逻辑或运算符 (||) 结果：\" + resultOr); // 逻辑非运算符 (!) // 如果条件为true，则结果为false；如果条件为false，则结果为true boolean resultNot = !condition1; System.out.println(\"逻辑非运算符 (!) 结果：\" + resultNot); } } 三元(三目)运算符 (关系表达式) ? 表达式1：表达式2； true执行1表达式，false执行2表达式 public class OperatorDemo04 { public static void main(String[] args) { //1. 定义两个int类型的变量a. b, 初始化值分别为10, 20 int a = 10, b = 20; //2. 通过三元运算符, 获取变量a和b的最大值. int max = a \u003c b ? b : a; //3. 将结果(最大值)打印到控制台上. System.out.println(max); } } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:8:1","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.8 流程控制","date":"2024-04-26","objectID":"/lang/java/20240426133928/:9:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.8.1 分支结构如果我们想某些代码是在满足条件的情况下, 才能被执行, 此时就需要用到选择结构了, 选择结构也叫分支结构, 主要分为以下两种: if语句, 主要用于范围的判断 switch.case语句, 主要用于固定值的判断. if分支 public class IfExample { public static void main(String[] args) { int num = 10; if (num \u003e 0) { System.out.println(\"数字是正数\"); } else if (num \u003c 0) { System.out.println(\"数字是负数\"); } else { System.out.println(\"数字是零\"); } } } switch分支 public class SwitchExample { public static void main(String[] args) { int dayOfWeek = 3; String dayName; switch (dayOfWeek) { case 1: dayName = \"星期一\"; break; case 2: dayName = \"星期二\"; break; case 3: dayName = \"星期三\"; break; case 4: dayName = \"星期四\"; break; case 5: dayName = \"星期五\"; break; case 6: dayName = \"星期六\"; break; case 7: dayName = \"星期日\"; break; default: dayName = \"无效的日期\"; break; } System.out.println(\"今天是：\" + dayName); } } switch 分支 case穿透，在switch语句中，如果case的后面不写break，将出现case穿透现象，也就是不会在判断下一个case的值，直接向后运行，直到遇到break，或者整体switch结束。 import java.util.Scanner; public class SwitchDemo08 { public static void main(String[] args) { Scanner sc = new Scanner(System.in); System.out.println(\"请录入一个月份: \"); int month = sc.nextInt(); switch (month) { case 12: case 1: case 2: System.out.println(\"冬季\"); break; case 3: case 4: case 5: System.out.println(\"春季\"); break; case 6: case 7: case 8: System.out.println(\"夏季\"); break; case 9: case 10: case 11: System.out.println(\"秋季\"); break; default: System.out.println(\"没有这样的日期\"); break; } } } } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:9:1","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.8.2 循环结构for循环 for(初始化条件1; 判断条件2; 控制条件3) { //循环体4; } for循环求1-100之前偶数和 public class ForDemo04 { public static void main(String[] args) { //1. 定义变量sum, 用来记录数据和. int sum = 0; //2. 通过for循环, 依次获取到1 - 100之间的数字. for (int i = 1; i \u003c= 100; i++) { //3. 判断当前遍历到的数字是否是偶数. if (i % 2 == 0) { //4. 走到这里, 说明是偶数, 累加给变量sum. sum += i; } } //5. 打印结果. System.out.println(\"1 - 100之间的偶数之和是: \" + sum); } } while循环 初始化条件1; while(判断条件2) { //循环体3; //控制条件4; } 例子 public class WhileDemo01 { public static void main(String[] args) { int i = 0; while(i \u003c 10) { System.out.println(\"Hello World!\"); i++; } } } 死循环：永远不结束的循环，循环的判断条件是true for(;;) { } while(true){} do {} while(true) 循环跳转: break continue public class ContinueExample { public static void main(String[] args) { for (int i = 1; i \u003c= 5; i++) { // 如果i为偶数，跳过本次循环 if (i % 2 == 0) { continue; } /*if (i==3) { break; }*/ System.out.println(\"当前数字是：\" + i); } } } 嵌套循环打印99乘法表 public class ForForDemo08 { public static void main(String[] args) { for (int i = 1; i \u003c= 9; i++) { //外循环控制行 for (int j = 1; j \u003c= i; j++) { //内循环控制列 //1 * 3 = 3 2 * 3 = 6 3 * 3 = 9 System.out.print(j + \" * \" + i + \" = \" + i * j + \"\\t\"); } System.out.println(); //内循环执行结束, 意味着一行打印完毕, 记得要换行. } } } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:9:2","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":" 前言：以初学着的身份，准备在该平台整理点最近学习的知识，方便后续查看相关的技术点，有兴趣的可以一块交流学习。目标用最少的文字，理解最多的知识。不说废话，字字珠玑。 ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:0:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.1 简介 强类型语言、开源、跨平台、多态、多线程、面向对象 完善的异常处理机制，大数据必备的语言 1995年出生，父亲 詹姆斯·高斯林(James Gosling)，2009年被Sun公司收购 兄弟版本 J2SE: 标准版, 也是其他两个版本的基础. 在JDK1.5的时候正式更名为: JavaSE. J2ME: 小型版, 一般用来研发嵌入式程序. 已经被Android替代了. 在JDK1.5的时候正式更名为: JavaME. J2EE: 企业版, 一般开发企业级互联网程序. 在JDK1.5的时候正式更名为: JavaEE ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:1:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.2 Java环境搭建 JDK和JRE区别 JRE: Java运行时环境(Java Runtime Environment) 运行Java的环境 JDK: Java development kit Java开发工具包，包含开发工具和JRE JVM：Java虚拟机(Java Virtual Machine) 和，Java运行环境里要有JVM 目录解释 bin: 存放的是编译器和工具 db: 存数数据 include: 编译本地方法. jre: Java运行时文件 lib: 存放类库文件 src.zip: 存放源代码的 对比这图片看能不能理解每个文件啥意思，不能理解，好好看看上面的介绍 怎么安装和配置path，不同的系统自己动手搜搜吧 ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:2:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.3 程序的开发步骤Java这种静态语言的的开发步骤一般分为 编写：编写源代码，在后缀名为.java的源文件中编写，用idea开发工具 编译:把源代码，编译成计算机能看懂的文件. javacv执行生成.class文件 执行:让计算机运行指定的代码程序 java 运行 直接整idea开发工具，来个hello world的例子 ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:3:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.4 idea开发工具 idea 下载地址 https://www.jetbrains.com/idea/ 激活方式 小程序 码叔资源 上找个激活码激活，或淘宝上买一个激活码 下载好后一路next安装，建议安装路径放D盘，软件是真大 ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:4:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.5用idea开发一个helloworld 新建一个空项目 效果如下 我这一个空文件夹，怎么和Java关联起来呢，要和JDK关联上，才能用写Java代码是吧 打开刚刚新建的项目 有个 项目结构这个东西后面用到的挺多，配置 重点讲讲 project modules libraries的区别 项目：可以执行你刚刚创建那个空项目的名称，SDK(用Java还是什么开发),编译器输出路径 模块：我们新建一个day01的模块，后面可以在这个项目下新增day02 day03的模块，你也可以每天建一个Java项目。 建完之后项目下新增了一个day01的模块，可以在该模块下写代码了(真不容易) 删除模块 删除模块只是这个项目中看不到该模块了，在文件夹中还是存在的 依赖库 ：我这个项目想用第三方的包，咋个办，在这导入就行了，例如连接MySQL的包 搞了这么久，idea上写代码试下吧,day01模块下 src目录下新建一个 Java源代码文件,输入下面的代码 public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World!\"); } } idea使用小tips，idea很强大，这代码太繁琐了不会写怎么办，idea会自动补全的 输入main 按tab键，main方法出来了，sout 按tab键 print语句出来了，更多idea的技巧有机会，单独出一篇文章 运行代码初体验,直接右键run(idea会先编译，然后执行)，终端中会打印出helloworld ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:5:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1 网站 小林coding (xiaolincoding.com)🥓 🍳刘沙河 (bigox.wiki) 🧇爱编程的大丙 (subingwen.cn) ","date":"2024-04-25","objectID":"/web/:1:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"\r2 常用官方API MySQL官方文档🍕 Hive官方文档🍔 官方API- Spark 2.4.8🍟 ","date":"2024-04-25","objectID":"/web/:2:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"\r3 工具类 在线Cron表达式生成器 (qqe2.com) RegExr: Learn, Build, \u0026 Test RegEx The-X 在线工具箱 Base64 解码 AES RAS 解码 加密 ","date":"2024-04-25","objectID":"/web/:3:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"\r4 影视 BTNULL 无名小站🍿 ","date":"2024-04-25","objectID":"/web/:4:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"\r5 图书 安娜的档案 (annas-archive.org)🧂 ","date":"2024-04-25","objectID":"/web/:5:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"Hive官方操作手册 ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:0:0","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r1 Hive特性 hive基于Hadoop，将结构化数据映射为一张数据库表，提供类SQL查询功能 Hive元数据存储,通常存储在关系形数据库中，Hive中的元数据包括(表的名称，列，分区，是否为外部表等属性，数据所在的路径) Hive支持MapReduce、Tez和Spark 三种计算引擎。 数据格式。数据格式用户定义，根据三个属性：列分隔符（通常为空格、”\\t”、”\\x001″）、行分隔符（”\\n”）以及读取文件数据的方法（Hive 中默认有三个文件格式 TextFile，SequenceFile 以及 RCFile） 数据更新。不支持对数据的更新，使用更新可以借助Impala引擎+kudu数据格式 索引。有hive版本支持建立索引，但是各种问题，不建议使用。 Hive中包含的数据模型，DB、Table，External Table，Partition，Bucket db：在hdfs中表现为hive.metastore.warehouse.dir目录下一个文件夹。 table：在hdfs中表现所属db目录下一个文件夹。 external table：与table类似，不过其数据存放位置可以在任意指定路径。 partition：在hdfs中表现为table目录下的子目录。 bucket：在hdfs中表现为同一个表目录下根据hash散列之后的多个文件。 ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:1:0","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2 Hive表类型","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:0","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.1 hive数据类型 Hive的基本数据类型有：TINYINT，SAMLLINT，INT，BIGINT，BOOLEAN，FLOAT，DOUBLE，STRING，TIMESTAMP(V0.8.0+)和BINARY(V0.8.0+)。 Hive的集合类型有：STRUCT，MAP和ARRAY。 Hive表：内部表、外部表、分区表和桶表。 表的元数据保存传统的数据库的表中，当前hive只支持Derby和MySQL数据库。 ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:1","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.2 Hive内部表Hive中的内部表和传统数据库中的表在概念上是类似的，Hive的每个表都有自己的存储目录，除了外部表外，所有的表数据都存放在配置在hive-site.xml文件的${hive.metastore.warehouse.dir}/table_name目录下。 -- 创建内部表 CREATE TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, grade STRING COMMOT '班级'）COMMONT '学生表' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORE AS TEXTFILE; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:2","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.3 Hive外部表被external修饰的为外部表（external table），外部表指向已经存在在Hadoop HDFS上的数据，除了在删除外部表时只删除元数据而不会删除表数据外，其他和内部表很像 -- 创建外部表 CREATE EXTERNAL TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, class STRING COMMOT '班级'）COMMONT '学生表' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORE AS SEQUENCEFILE LOCATION '/usr/test/data/students.txt'; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:3","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.4 Hive分区表分区表的每一个分区都对应数据库中相应分区列的一个索引，分区表中的每一个分区对应一个目录文件，即同一个分区的数据存放一个目录下面，所以分区表如果很大，指定分区会速度快很多 比如说，分区表partitinTable有包含nation(国家)、ds(日期)和city(城市)3个分区，其中nation = china，ds = 20130506，city = Shanghai则对应HDFS上的目录为： /datawarehouse/partitinTable/nation=china/city=Shanghai/ds=20130506/。 -- 创建分区表 CREATE TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, class STRING COMMOT '班级'）COMMONT '学生表' PARTITIONED BY (ds STRING,country STRING) -- 分区的列不属于建表的字段中 ROW FORMAT DELIMITED --分隔符 FIELDS TERMINATED BY ',' -- 结束换行符 STORE AS SEQUENCEFILE -- 数据存储格式 ; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:4","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.5 Hive分桶表对指定列进行HASH，根据hash值进行切分数据，不同hash结果的数据写到每个桶对应的文件目录中 -- 创建分桶表 CREATE TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, class STRING COMMOT '班级',score SMALLINT COMMOT '总分'）COMMONT '学生表' PARTITIONED BY (ds STRING,country STRING) CLUSTERED BY(user_no) SORTED BY(score) INTO 32 BUCKETS -- 分桶 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORE AS SEQUENCEFILE; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:5","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.6 Hive视图逻辑数据结构，将查询结果作为视图，简化查询操作 CREATE VIEW employee_skills AS SELECT name, skills_score['DB'] AS DB, skills_score['Perl'] AS Perl, skills_score['Python'] AS Python, skills_score['Sales'] as Sales, skills_score['HR'] as HR FROM employee; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:6","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.7 总结表的创建官方标准 LanguageManual DDL -创建表的语法结构 CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables) -- 用另外一个表的结构来创建 CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; 删除表 DROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later) 清空表 TRUNCATE [TABLE] table_name [PARTITION partition_spec]; partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...) ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:7","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" HDFS ","date":"2024-04-24","objectID":"/bigdata/hadoop/a9bc224/:0:0","tags":["Hadoop"],"title":"HDFS文件读写过程","uri":"/bigdata/hadoop/a9bc224/"},{"categories":null,"content":"\r1 文件的写入过程HDFS把文件的数据划分成若干个块(Block)， 每个Block存放在一组 DataNode上，Namenode负责维护文件–\u003eBlock(命令空间映射)和Block–\u003eDatanode(数据块映射) 写入流程 Client发起文件上传请求，通过RPC与NameNode建立通讯，NameNode检查目标文件是否已存在，父目录是否存在，返回是否可以上传 Client请求确定第一个block 应该传输到哪些Datanode服务器上 NameNode根据配置文件指定的备份数量以及机架感知原理进行文件分配，返回可用的 DataNode地址，如：A，B，C 备份数量：默认在HDFS上存放三份，本地一份、同机架内其他节点一份、不同机架节点一份 Client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline ），A 收到请求会继续调用 B，然后 B 调用 C，将整个 pipeline 建立完成， 后逐级返回 client； Client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存），以 packet 为单位（默认64K），A 收到一个 packet 就会传给 B，B 传给 C。A 每传一个 packet 会放入一个应答队列等待应答； 数据被分割成一个个 packet 数据包在 pipeline 上依次传输，在 pipeline 反方向上， 逐个发送 ack（命令正确应答），最终由 pipeline 中第一个 DataNode 节点 A 将 pipelineack 发送给 Client； 当一个 block 传输完成之后，Client 再次请求 NameNode 上传第二个 block，重复步骤 2； ","date":"2024-04-24","objectID":"/bigdata/hadoop/a9bc224/:1:0","tags":["Hadoop"],"title":"HDFS文件读写过程","uri":"/bigdata/hadoop/a9bc224/"},{"categories":null,"content":"\r2 HDFS文件读取过程 Client远程调用请求NameNode，获取文件块位置列表 NameNode会视情况返回文件的部分或者全部block列表；对于每个block，NameNode返回含副本的所有DataNode 地址；计算传输最快最优的DataNode 返回的DataNode地址，会按照集群拓扑结构计算客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 STALE，这样的排靠后； Client 选取排序靠前的 DataNode建立输入流，如果客户端本身就是DataNode，那么将从本地直接获取数据(短路读取特性)； 在选定的DataNode上读取该Block的数据 当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表； 读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的DataNode 继续读。 read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回Client请求包含块的DataNode地址，并不是返回请求块的数据； 最终读取来所有的 block 会合并成一个完整的最终文件。 总结：串行写入，数据包先发给节点A，然后节点A发送给B，B在给C 并行读取，并行读取block所在的节点，最后合并 ","date":"2024-04-24","objectID":"/bigdata/hadoop/a9bc224/:2:0","tags":["Hadoop"],"title":"HDFS文件读写过程","uri":"/bigdata/hadoop/a9bc224/"},{"categories":["project"],"content":"\r1 HDFS 的命令行使用Hadoop 命令行官方doc hadoop fs: 使用面最广，可以操作任务文件系统，包括本地文件系统、HDFS、FTP、S3等 hdfs dfs: 只能操作HDFS文件系统。 # 下面两种效果是一样的 hdfs dfs -ls /user/hive hadoop fs -ls /user/hive/ 如果操作HDFS文件系统，推荐使用 hdfs dfs,如果需要操作其他系统文件，使用hadoop fs命令 help 格式: hdfs dfs -help 操作命令 作用: 查看某一个操作命令的参数信息 ls 格式：hdfs dfs -ls URI 作用：类似于Linux的ls命令，显示文件列表 lsr 格式 : hdfs dfs -lsr URI 作用 : 在整个目录下递归执行ls, 与UNIX中的ls-R类似 mkdir 格式 ： hdfs dfs -mkdir [-p] \u003cpaths\u003e 作用 : 以\u003cpaths\u003e中的URI作为参数，创建目录。使用-p参数可以递归创建目录 put 格式 ： hdfs dfs -put \u003clocalsrc \u003e ... \u003cdst\u003e 作用 ： 将单个的源文件src或者多个源文件srcs从本地文件系统拷贝到目标文件系统中（\u003cdst\u003e对应的路径）。也可以从标准输入中读取输入，写入目标文件系统中 hdfs dfs -put /rooot/bigdata.txt /dir1 moveFromLocal 格式： hdfs dfs -moveFromLocal \u003clocalsrc\u003e \u003cdst\u003e 作用: 和put命令类似，但是源文件localsrc拷贝之后自身被删除 hdfs dfs -moveFromLocal /root/bigdata.txt / copyFromLocal 格式: hdfs dfs -copyFromLocal \u003clocalsrc\u003e ... \u003cdst\u003e 作用: 从本地文件系统中拷贝文件到hdfs路径去 appendToFile 格式: hdfs dfs -appendToFile \u003clocalsrc\u003e ... \u003cdst\u003e 作用: 追加一个或者多个文件到hdfs指定文件中.也可以从命令行读取输入. hdfs dfs -appendToFile a.xml b.xml /big.xml moveToLocal 在 hadoop 2.6.4 版本测试还未未实现此方法 格式：hadoop dfs -moveToLocal [-crc] \u003csrc\u003e \u003cdst\u003e 作用：将本地文件剪切到 HDFS get 格式 hdfs dfs -get [-ignorecrc ] [-crc] \u003csrc\u003e \u003clocaldst\u003e 作用：将文件拷贝到本地文件系统。 CRC 校验失败的文件通过-ignorecrc选项拷贝。 文件和CRC校验可以通过-CRC选项拷贝 hdfs dfs -get /bigdata.txt /export/servers getmerge 格式: hdfs dfs -getmerge \u003csrc\u003e \u003clocaldst\u003e 作用: 合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,... copyToLocal 格式: hdfs dfs -copyToLocal \u003csrc\u003e ... \u003clocaldst\u003e 作用: 从hdfs拷贝到本地 mv 格式 ： hdfs dfs -mv URI \u003cdest\u003e 作用： 将hdfs上的文件从原路径移动到目标路径（移动之后文件删除），该命令不能跨文件系统 hdfs dfs -mv /dir1/bigdata.txt /dir2 rm 格式： hdfs dfs -rm [-r] 【-skipTrash】 URI 【URI 。。。】 作用： 删除参数指定的文件，参数可以有多个。 此命令只删除文件和非空目录。 如果指定-skipTrash选项，那么在回收站可用的情况下，该选项将跳过回收站而直接删除文件； 否则，在回收站可用时，在HDFS Shell 中执行此命令，会将文件暂时放到回收站中。 hdfs dfs -rm -r /dir1 cp 格式: hdfs dfs -cp URI [URI ...] \u003cdest\u003e 作用： 将文件拷贝到目标路径中。如果\u003cdest\u003e 为目录的话，可以将多个文件拷贝到该目录下。 -f 选项将覆盖目标，如果它已经存在。 -p 选项将保留文件属性（时间戳、所有权、许可、ACL、XAttr）。 hdfs dfs -cp /dir1/a.txt /dir2/bigdata.txt cat hdfs dfs -cat URI [uri ...] 作用：将参数所指示的文件内容输出到stdout hdfs dfs -cat /bigdata.txt tail 格式: hdfs dfs -tail path 作用: 显示一个文件的末尾 text 格式:hdfs dfs -text path 作用: 以字符形式打印一个文件的内容 chmod 格式:hdfs dfs -chmod [-R] URI[URI ...] 作用：改变文件权限。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 hdfs dfs -chmod -R 777 /bigdata.txt chown 格式: hdfs dfs -chmod [-R] URI[URI ...] 作用： 改变文件的所属用户和用户组。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 hdfs dfs -chown -R hadoop:hadoop /bigdata.txt df 格式: hdfs dfs -df -h path 作用: 统计文件系统的可用空间信息 du 格式: hdfs dfs -du -s -h path 作用: 统计文件夹的大小信息 count 格式: hdfs dfs -count path 作用: 统计一个指定目录下的文件节点数量 setrep expunge (慎用) 格式: hdfs dfs -setrep num filePath 作用: 设置hdfs中文件的副本数量 注意: 即使设置的超过了datanode的数量,副本的数量也最多只能和datanode的数量是一致的 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:1:0","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":"\r2 hdfs的高级使用命令","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:0","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":"\r2.1 HDFS文件限额配置在多人共用HDFS的环境下，配置设置非常重要。特别是在 Hadoop 处理大量资料的环境，如果没有配额管理，很容易把所有的空间用完造成别人无法存取。HDFS 的配额设定是针对目录而不是针对账号，可以让每个账号仅操作某一个目录，然后对目录设置配置。 HDFS 文件的限额配置允许我们以文件个数，或者文件大小来限制我们在某个目录下上传的文件数量或者文件内容总量，以便达到我们类似百度网盘网盘等限制每个用户允许上传的最大的文件的量。 hdfs dfs -count -q -h /user/root/dir1 #查看配额信息 2.1.1 数量限额 hdfs dfs -mkdir -p /user/root/dir #创建hdfs文件夹 hdfs dfsadmin -setQuota 2 dir # 给该文件夹下面设置最多上传两个文件，发现只能上传一个文件 hdfs dfsadmin -clrQuota /user/root/dir # 清除文件数量限制 2.1.2 空间大小限额在设置空间配额时，设置的空间至少是 block_size * 3 大小 hdfs dfsadmin -setSpaceQuota 4k /user/root/dir # 限制空间大小4KB hdfs dfs -put /root/a.txt /user/root/dir # 生成任意大小的文件 dd if=/dev/zero of=1.txt bs=1M count=2 #生成2M的文件 # 清除空间配额限制 hdfs dfsadmin -clrSpaceQuota /user/root/dir ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:1","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":"\r2.2 HDFS 的安全模式安全模式是hadoop的一种保护机制，用于保证集群中的数据块的安全性。当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。 假设我们设置的副本数（即参数dfs.replication）是3，那么在datanode上就应该有3个副本存在，假设只存在2个副本，那么比例就是2/3=0.666。hdfs默认的副本率0.999。我们的副本率0.666明显小于0.999，因此系统会自动的复制副本到其他dataNode，使得副本率不小于0.999。如果系统中有5个副本，超过我们设定的3个副本，那么系统也会删除多于的2个副本。 在安全模式状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在，当整个系统达到安全标准时，HDFS自动离开安全模式。30s 安全模式操作命令 hdfs dfsadmin -safemode get #查看安全模式状态 hdfs dfsadmin -safemode enter #进入安全模式 hdfs dfsadmin -safemode leave #离开安全模式 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:2","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["开发"],"content":"\rHDFS HDFS概述 Hadoop 分布式系统框架中，首要的基础功能就是文件系统，在 Hadoop 中使用 FileSystem 这个抽象类来表示我们的文件系统，这个抽象类下面有很多子实现类，究竟使用哪一种，需要看我们具体的实现类，在我们实际工作中，用到的最多的就是HDFS(分布式文件系统)以及LocalFileSystem(本地文件系统)了。 在现代的企业环境中，单机容量往往无法存储大量数据，需要跨机器存储。统一管理分布在集群上的文件系统称为分布式文件系统。 HDFS（Hadoop Distributed File System）是 Hadoop 项目的一个子项目。是 Hadoop 的核心组件之一， Hadoop 非常适于存储大型数据 (比如 TB 和 PB)，其就是使用 HDFS 作为存储系统. HDFS 使用多台计算机存储文件，并且提供统一的访问接口，像是访问一个普通文件系统一样使用分布式文件系统。 HDFS架构 HDFS是一个主/从（Mater/Slave）体系结构，由三部分组成： NameNode 和 DataNode 以及 SecondaryNamenode： ● NameNode 负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。 ● DataNode 负责管理用户的文件数据块，每一个数据块都可以在多个 DataNode 上存储多个副本，默认为3个。 ● Secondary NameNode 用来监控 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS 元数据的快照。最主要作用是辅助 NameNode 管理元数据信息。 HDFS的特性 首先，它是一个文件系统，用于存储文件，通过统一的命名空间目录树来定位文件； 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 master/slave 架构（主从架构） HDFS 采用 master/slave 架构。一般一个 HDFS 集群是有一个 Namenode 和一定数目的 Datanode 组成。Namenode 是 HDFS 集群主节点，Datanode 是 HDFS 集群从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。 分块存储(Block) HDFS 中的文件在物理上是分块存储（block）的，块的大小可以通过配置参数来规定，默认大小在 hadoop2.x 版本中是 128M。 名字空间（NameSpace） HDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。 Namenode 负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被 Namenode 记录下来。 HDFS 会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。 NameNode 元数据管理 我们把目录结构及文件分块位置信息叫做元数据。NameNode 负责维护整个 HDFS 文件系统的目录树结构，以及每一个文件所对应的 block 块信息（block 的 id，及所在的 DataNode 服务器）。 DataNode 数据存储 文件的各个 block 的具体存储管理由 DataNode 节点承担。每一个 block 都可以在多个 DataNode 上。DataNode 需要定时向 NameNode 汇报自己持有的 block 信息。 存储多个副本（副本数量也可以通过参数设置 dfs.replication，默认是 3） 副本机制 为了容错，文件的所有 block 都会有副本。每个文件的 block 大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。 一次写入，多次读出 HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的修改。 正因为如此，HDFS 适合用来做大数据分析的底层存储服务，并不适合用来做网盘等应用，因为修改不方便，延迟大，网络开销大，成本太高。 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E6%A6%82%E8%A7%88/:0:0","tags":["Hadoop"],"title":"Hadoop概览","uri":"/bigdata/hadoop/hadoop%E6%A6%82%E8%A7%88/"},{"categories":["开发"],"content":"Configuration - Spark 2.4.8 配置参数 ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:0:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":["开发"],"content":"\rApache Spark A Unified engine for large-scale data analytics Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing. ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:1:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":["开发"],"content":"\rDownloadingGet Spark from the downloads page of the project website. This documentation is for Spark version 3.5.1. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath. Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI. If you’d like to build Spark from source, visit Building Spark. ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:2:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"\rIntroductionThis is bold text, and this is emphasized text. Visit the Hugo website! ","date":"2024-04-20","objectID":"/posts/my-first-post/:1:0","tags":null,"title":"My First Post","uri":"/posts/my-first-post/"},{"categories":null,"content":"\r常用数据库的读写方式A blog (a truncation of “weblog”) is an informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (posts). Posts are typically displayed in reverse chronological order so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual,[citation needed] occasionally of a small group, and often covered a single subject or topic. In the 2010s, “multi-author blogs” (MABs) emerged, featuring the writing of multiple authors and sometimes professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other “microblogging” systems helps integrate MABs and single-author blogs into the news media. Blog can also be used as a verb, meaning to maintain or add content to a blog. ","date":"2024-02-20","objectID":"/posts/second_post/:1:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":null,"content":"\r标题二","date":"2024-02-20","objectID":"/posts/second_post/:2:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":null,"content":"\r标题三","date":"2024-02-20","objectID":"/posts/second_post/:3:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":["开发"],"content":"\rPythonpython第二篇文章扽扽 ","date":"2023-02-20","objectID":"/lang/python/first/:1:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":"\r标题二","date":"2023-02-20","objectID":"/lang/python/first/:2:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":"\r标题三","date":"2023-02-20","objectID":"/lang/python/first/:3:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":"\rPythonpython第一篇文章扽扽 ","date":"2023-02-20","objectID":"/lang/python/second/:1:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":"\r标题二","date":"2023-02-20","objectID":"/lang/python/second/:2:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":"\r标题三","date":"2023-02-20","objectID":"/lang/python/second/:3:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":"\r常用数据库的读写方式A blog (a truncation of “weblog”) is an informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (posts). Posts are typically displayed in reverse chronological order so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual,[citation needed] occasionally of a small group, and often covered a single subject or topic. In the 2010s, “multi-author blogs” (MABs) emerged, featuring the writing of multiple authors and sometimes professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other “microblogging” systems helps integrate MABs and single-author blogs into the news media. Blog can also be used as a verb, meaning to maintain or add content to a blog. ","date":"2023-02-20","objectID":"/posts/first_post/:1:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"},{"categories":["开发"],"content":"\r标题二","date":"2023-02-20","objectID":"/posts/first_post/:2:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"},{"categories":["开发"],"content":"\r标题三","date":"2023-02-20","objectID":"/posts/first_post/:3:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"}]