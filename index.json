[{"categories":["github"],"content":"本地拉取github老是失败，可通过全局配置代理，或只给github.com配置代理 前言：本地拉取github老是失败，可通过全局配置代理，或只给github.com配置代理 git 配置代理","date":"2025-04-25","objectID":"/lang/git/20250425152252/:0:0","tags":null,"title":"多平台GitHub配置代理","uri":"/lang/git/20250425152252/"},{"categories":["github"],"content":"\rwindows 设置代理","date":"2025-04-25","objectID":"/lang/git/20250425152252/:1:0","tags":null,"title":"多平台GitHub配置代理","uri":"/lang/git/20250425152252/"},{"categories":["github"],"content":"\rhttp || https协议 //设置全局代理 git config --global https.proxy http://127.0.0.1:7897 git config --global https.proxy https://127.0.0.1:7897 // socks git config --global http.proxy socks5://127.0.0.1:7897 git config --global https.proxy socks5://127.0.0.1:7897 //只对github.com使用代理，其他仓库不走代理 git config --global http.https://github.com.proxy socks5://127.0.0.1:7897 git config --global https.https://github.com.proxy socks5://127.0.0.1:7897 //取消github代理 git config --global --unset http.https://github.com.proxy git config --global --unset https.https://github.com.proxy //取消全局代理 git config --global --unset http.proxy git config --global --unset https.proxy ","date":"2025-04-25","objectID":"/lang/git/20250425152252/:1:1","tags":null,"title":"多平台GitHub配置代理","uri":"/lang/git/20250425152252/"},{"categories":["github"],"content":"\rSSH协议 //对于使用git@协议的，可以配置socks5代理 //在~/.ssh/config 文件后面添加几行，没有可以新建一个 //socks5 Host github.com User git ProxyCommand connect -S 127.0.0.1:7897 %h %p //http || https Host github.com User git ProxyCommand connect -H 127.0.0.1:7897 %h %p ","date":"2025-04-25","objectID":"/lang/git/20250425152252/:1:2","tags":null,"title":"多平台GitHub配置代理","uri":"/lang/git/20250425152252/"},{"categories":["github"],"content":"\rWSL2设置代理在 Ubuntu 子系统中，通过 cat /etc/resolv.conf 查看 DNS 服务器 IP cat /etc/resolv.conf # This file was automatically generated by WSL. To stop automatic generation of this file, add the following entry to /etc/wsl.conf: # [network] # generateResolvConf = false nameserver 172.23.64.1 其实上面地址就是windows下面这个ip地址 win11-\u003e设置-\u003e网络和Internet-\u003e高级网络设置-\u003e硬件和连接属性 名称：vEthernet (WSL) ipv4地址：172.23.64.1/20 我们要为WSL配置ssh代理和http代理 touch ~/.ssh/config vim ~/.ssh/config Host github.com HostName github.com User git # 走 socks5 代理 ProxyCommand nc -v -x 172.28.32.1:7897 %h %p 使用下面命令检查ssh代理配置是否成功 ssh -T github.com Connection to github.com 22 port [tcp/ssh] succeeded! Hi wq-zhijun! You've successfully authenticated, but GitHub does not provide shell access. 可以将上面ip地址配置代理写入到.bashrc文件中，这样就可以自己用户开机永久生效， 另外apt要单独在/etc/apt/apt.conf设置代理 export u_host=`cat /etc/resolv.conf|grep nameserver|awk '{print $2}'` sed -i \"/.*ProxyCommand*/c\\ ProxyCommand nc -v -x $u_host:7897 %h %p\" ~/.ssh/config proxy () { export ALL_PROXY=\"http://$u_host:7897\" export all_proxy=\"http://$u_host:7897\" export {http,https,ftp}_proxy=$ALL_PROXY export {HTTP,HTTPS,FTP}_PROXY=$ALL_PROXY echo -e \"Acquire::http::Proxy \\\"http://$u_host:7897\\\";\" | sudo tee -a /etc/apt/apt.conf \u003e /dev/null echo -e \"Acquire::https::Proxy \\\"http://$u_host:7897\\\";\" | sudo tee -a /etc/apt/apt.conf \u003e /dev/null curl ip.gs } proxy # default call proxy noproxy () { unset ALL_PROXY unset all_proxy sudo sed -i -e '/Acquire::http::Proxy/d' /etc/apt/apt.conf sudo sed -i -e '/Acquire::https::Proxy/d' /etc/apt/apt.conf curl ip.gs } 7897端口 是clash for windows端口，另外将 Allow LAN 打开。 还需要将防火墙打开 win+r 输入 control到如下目录 控制面板\\系统和安全\\Windows Defender 防火墙 放行clash* ","date":"2025-04-25","objectID":"/lang/git/20250425152252/:2:0","tags":null,"title":"多平台GitHub配置代理","uri":"/lang/git/20250425152252/"},{"categories":["github"],"content":"\rgit push失败或者卡住不动问题原因是有的节点22端口被服务端封锁了，要么改服务端端口要么改自己本地更换443端口（~/.ssh/config) # 可以先使用如下测试下 ssh -T -p 443 git@ssh.github.com # Hi USERNAME! You've successfully authenticated, but GitHub does not provide shell access. # 更换端口后再测试下 ssh -T git@github.com # Hi USERNAME! You've successfully authenticated, but GitHub does not provide shell access. # .ssh目录的config文件添加如下内容 vim ~/.ssh/config Host github.com HostName github.com User git # set socks5 proxy ProxyCommand nc -v -x 172.23.64.1:7897 %h %p ","date":"2025-04-25","objectID":"/lang/git/20250425152252/:3:0","tags":null,"title":"多平台GitHub配置代理","uri":"/lang/git/20250425152252/"},{"categories":["github"],"content":"\r修改配置文件的方式 windows系统的配置 文件在 C:\\Users\\admin\\.gitconfig 配置文件如下 [http \"https://github.com\"] proxy = http://127.0.0.1:7897 [https \"https://github.com\"] proxy = https://127.0.0.1:7897 ","date":"2025-04-25","objectID":"/lang/git/20250425152252/:4:0","tags":null,"title":"多平台GitHub配置代理","uri":"/lang/git/20250425152252/"},{"categories":null,"content":"\rsqoop介绍 Sqoop 是一个设计用于在 Hadoop 和关系数据库或大型机之间传输数据的工具。您可以使用 Sqoop 将数据从关系数据库管理系统 (RDBMS)（例如 MySQL 或 Oracle）或大型机导入 Hadoop 分布式文件系统 (HDFS)，在 Hadoop MapReduce 中转换数据，然后将数据导出回 RDBMS 。 Hadoop生态包括： HDFS，Hive，Hbase等 RDBMS体系包括：Mysql、Oracle、DB2等 sqoop是一些工具的集合, 使用的语法如下 sqoop tool-name [tool-arguments] ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:1:0","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r命令工具可用的命令工具如下 可以使用 –help 查看命令工具的使用方法 sqoop import --help sqoop help import 命令 作用描述 使用方法 codegen 生成与数据库记录交互的代码 sqoop codegen --connect \u003c连接字符串\u003e --table \u003c表名\u003e create-hive-table 将表定义导入到 Hive 中 sqoop create-hive-table --connect \u003c连接字符串\u003e --table \u003c表名\u003e eval 执行 SQL 语句并显示结果 sqoop eval --connect \u003c连接字符串\u003e --query \u003cSQL 语句\u003e export 将 HDFS 目录导出到数据库表 sqoop export --connect \u003c连接字符串\u003e --table \u003c表名\u003e --export-dir \u003cHDFS 目录\u003e help 列出可用命令 sqoop help import 将数据库表导入到 HDFS sqoop import --connect \u003c连接字符串\u003e --table \u003c表名\u003e import-all-tables 将数据库中的所有表导入到 HDFS sqoop import-all-tables --connect \u003c连接字符串\u003e import-mainframe 将主机服务器上的数据集导入到 HDFS sqoop import-mainframe --connect \u003c连接字符串\u003e --dataset \u003c数据集\u003e job 使用已保存的作业 sqoop job --create \u003c作业名\u003e --import --connect \u003c连接字符串\u003e --table \u003c表名\u003e list-databases 列出服务器上的可用数据库 sqoop list-databases --connect \u003c连接字符串\u003e list-tables 列出数据库中的可用表 sqoop list-tables --connect \u003c连接字符串\u003e merge 合并增量导入的结果 sqoop merge --new-data \u003c新数据目录\u003e --onto \u003c旧数据目录\u003e metastore 运行独立的 Sqoop 元数据存储服务 sqoop metastore version 显示版本信息 sqoop version ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:2:0","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\rimport 功能 将关系型数据表导入到HDFS中，可以设置存储格式为Avro 或者 SequenceFiles sqoop import --help 查看该命令工具的用法 参数可以按功能来分组，包括：Common参数、Hive参数、Import control 参数、HBase 参数、HCatalog 参数、Accumulo 参数、 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:0","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\rcommon参数 Argument Description --connect \u003cjdbc-uri\u003e Specify JDBC connect string --connection-manager \u003cclass-name\u003e Specify connection manager class to use --driver \u003cclass-name\u003e Manually specify JDBC driver class to use --hadoop-mapred-home \u003cdir\u003e Override $HADOOP_MAPRED_HOME --help Print usage instructions --password-file Set path for a file containing the authentication password -P Read password from console --password \u003cpassword\u003e Set authentication password --username \u003cusername\u003e Set authentication username --verbose Print more information while working --connection-param-file \u003cfilename\u003e Optional properties file that provides connection parameters --relaxed-isolation Set connection transaction isolation to read uncommitted for the mappers. ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:1","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r验证参数可以验证同步之后的表的数据量是否一致 参数 描述 --validate 启用复制数据的验证，仅支持单表复制。 --validator \u003cclass-name\u003e 指定要使用的验证器类。 --validation-threshold \u003cclass-name\u003e 指定要使用的验证阈值类。 --validation-failurehandler \u003cclass-name\u003e 指定要使用的验证失败处理程序类。 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:2","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\rimport control 参数 参数 描述 --append 将数据附加到 HDFS 中的现有数据集 --as-avrodatafile 将数据导入 Avro 数据文件 --as-sequencefile 将数据导入 SequenceFiles --as-textfile 以纯文本形式导入数据（默认） --as-parquetfile 将数据导入 Parquet 文件 --boundary-query \u003cstatement\u003e 用于创建分割的边界查询 --columns \u003ccol,col,col…\u003e 从表中导入的列 --delete-target-dir 如果存在则删除导入目标目录 --direct 如果数据库存在，则使用直接连接器 --fetch-size \u003cn\u003e 一次从数据库读取的条目数。 --inline-lob-limit \u003cn\u003e 设置内联 LOB 的最大大小 -m,--num-mappers \u003cn\u003e 使用_n个_map任务并行导入 -e,--query \u003cstatement\u003e 导入的结果_statement_。 --split-by \u003ccolumn-name\u003e 用于分割工作单元的表格列。不能与 --autoreset-to-one-mapper选项一起使用。 --split-limit \u003cn\u003e 每个分割大小的上限。这仅适用于整数和日期列。对于日期或时间戳字段，以秒为单位计算。 --autoreset-to-one-mapper 如果表没有主键且未提供拆分列，则导入应使用一个映射器。不能与 --split-by \u003ccol\u003e选项一起使用。 --table \u003ctable-name\u003e 表格阅读 --target-dir \u003cdir\u003e HDFS 目标目录 --temporary-rootdir \u003cdir\u003e 导入期间创建的临时文件的 HDFS 目录（覆盖默认的“_sqoop”） --warehouse-dir \u003cdir\u003e 表目标的 HDFS 父级 --where \u003cwhere clause\u003e 导入期间使用的 WHERE 子句 -z,--compress 启用压缩 --compression-codec \u003cc\u003e 使用 Hadoop 编解码器（默认 gzip），parquet格式需要加这个配置 --null-string \u003cnull-string\u003e 为字符串列写入空值的字符串 --null-non-string \u003cnull-string\u003e 对于非字符串列，要写入空值的字符串 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:3","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r增量导入参数 参数 描述 --check-column (col) 指定在确定要导入哪些行时要检查的列。（该列不应为 CHAR/NCHAR/VARCHAR/VARNCHAR/LONGVARCHAR/LONGNVARCHAR 类型） --incremental (mode) 指定 Sqoop 如何确定哪些行是新行。include和 的mode 合法 值。 append``lastmodified --last-value (value) 指定上次导入的检查列的最大值。 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:4","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r输出行格式化参数，写入到hdfs中的行数据 参数 描述 --enclosed-by \u003cchar\u003e 设置必填字段括字符 --escaped-by \u003cchar\u003e 设置转义字符 --fields-terminated-by \u003cchar\u003e 设置字段分隔符，hive默认分隔符 是‘\\001’, 如果行数据以，分割，需设置这个 --lines-terminated-by \u003cchar\u003e 设置行尾字符 --mysql-delimiters 使用 MySQL 的默认分隔符集：字段：, 行：\\n 转义符：\\ 可选封闭符：' --optionally-enclosed-by \u003cchar\u003e 设置字段包围字符 支持的转义字符 \\b (backspace) \\n (newline) \\r (carriage return) \\t (tab) \\\" (double-quote) \\\\' (single-quote) \\\\ (backslash) \\0 (NUL) ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:5","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r输入解析参数 参数 描述 --input-enclosed-by \u003cchar\u003e 设置必填字段 --input-escaped-by \u003cchar\u003e 设置输入转义字符 --input-fields-terminated-by \u003cchar\u003e 设置输入字段分隔符 --input-lines-terminated-by \u003cchar\u003e 设置输入行尾字符 --input-optionally-enclosed-by \u003cchar\u003e 设置字段包围字符 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:6","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\rhive参数(常用) 参数 描述 --hive-home \u003cdir\u003e 覆盖$HIVE_HOME --hive-import 将表导入 Hive（如果未设置分隔符，则使用 Hive 的默认分隔符。） --hive-overwrite 覆盖 Hive 表中的现有数据。 --create-hive-table true,目标表存在会报错，默认 false。 --hive-table \u003ctable-name\u003e 设置导入到 Hive 时使用的表名。 --hive-drop-import-delims 导入到 Hive 时从字符串字段中 删除_\\n_、\\r_和\\01 。_ --hive-delims-replacement 导入 Hive 时，将字符串字段中的 \\n、\\r_和\\01_ 替换为用户定义的字符串。 --hive-partition-key 要进行分区的 Hive 字段的名称 --hive-partition-value \u003cv\u003e 在此作业中，作为导入到 Hive 中的分区键的字符串值。 --map-column-hive \u003cmap\u003e 覆盖配置列从 SQL 类型到 Hive 类型的默认映射。如果在此参数中指定逗号，请使用 URL 编码的键和值，例如，使用 DECIMAL(1%2C%201) 而不是 DECIMAL(1, 1)。 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:7","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\rHBase 参数 --column-family \u003cfamily\u003e 设置导入的目标列族 --hbase-create-table 如果指定，则创建缺失的 HBase 表 --hbase-row-key \u003ccol\u003e 指定使用哪个输入列作为行键 如果输入表包含复合 则 \u003ccol\u003e 必须采用 逗号分隔的复合键列表 属性 --hbase-table \u003ctable-name\u003e 指定要用作目标的 HBase表（而不是 HDFS） --hbase-bulkload 启用批量加载 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:8","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r使用例子\rmysql数据导入到hive，txt格式 分区表 sqoop import --connect jdbc:mysql://192.168.8.1:3306/{mysql_db}?tinyInt1isBit=false \\ --username user001 \\ --password passwd001 \\ --query \"{query}\" \\ --fields-terminated-by ',' \\ --hive-drop-import-delims \\ --null-string '\\\\\\\\N' \\ --null-non-string '\\\\\\\\N' \\ --lines-terminated-by \"\\\\n\" \\ --hive-overwrite \\ --hive-import \\ --target-dir /tmp/{hive_table} \\ --delete-target-dir \\ # 分区相关 --hive-partition-key {partition_key} \\ --hive-partition-value {partition_value} \\ --hive-database {hive_db} \\ --hive-table {hive_table} \\ --split-by '{split}' \\ -m {m} \\ mysql导入到hive，parquet格式 sqoop import -D mapred.job.name=ETL-${table_name} \\ --connect jdbc:mysql://192.168.8.61:3306/CN_Proj_DB?tinyInt1isBit=false \\ --username user001 \\ --password passwd001 \\ --query \"select * from ${table_name} where 1=1 and \\$CONDITIONS\" \\ --fields-terminated-by '\\001' \\ --hive-drop-import-delims \\ --delete-target-dir \\ --null-string '\\\\N' \\ --null-non-string '\\\\N' \\ --lines-terminated-by \"\\n\" \\ --target-dir /yyq_test/${table_name} \\ --hive-import \\ --hive-database bigworktest \\ --hive-table ${table_name} \\ --split-by id \\ -m 3 \\ # 压缩格式 parquet文件 --compress \\ --compression-codec org.apache.hadoop.io.compress.SnappyCodec \\ --hive-overwrite \\ --as-parquetfile mysql导入到hdfs文件 sqoop import \\ --connect jdbc:mysql://192.168.109.1:3306/userdb \\ --username root \\ --password root \\ --delete-target-dir \\ --fields-terminated-by '\\t' # 分隔符 --target-dir /sqoopresult \\ # 导入的hdfs目录 --table emp --m 1 mysql增量导入append模式 sqoop import \\ --connect jdbc:mysql://192.168.109.1:3306/userdb \\ --username root --password root \\ --table emp --m 1 \\ --target-dir /appendresult \\ --incremental append \\ --check-column id \\ # 主键 --last-value 1205 # 追加的id Lastmodified模式 按照时间戳字段 sqoop import \\ --connect jdbc:mysql://192.168.109.1:3306/userdb \\ --username root \\ --password root \\ --table customertest \\ --target-dir /lastmodifiedresult \\ --check-column last_mod \\ --incremental lastmodified \\ --last-value \"2019-09-03 22:59:45\" \\ --m 1 \\ # --merge-key id 修改旧的，新增新的 --append ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:3:9","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\rimport-all-tables 将一组表从关系型数据库导入到HDFS中 语法 sqoop import-all-tables (generic-args) (import-args) 每个组的参数参考官方文档 Sqoop User Guide (v1.4.7) (apache.org) ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:4:0","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\rexport HDFS数据 到 RDBMS 语法 sqoop export (generic-args) (export-args) ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:5:0","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\rcommon参数 争论 描述 --connect \u003cjdbc-uri\u003e 指定 JDBC 连接字符串 --connection-manager \u003cclass-name\u003e 指定要使用的连接管理器类 --driver \u003cclass-name\u003e 手动指定要使用的 JDBC 驱动程序类 --hadoop-mapred-home \u003cdir\u003e 覆盖 $HADOOP_MAPRED_HOME --help 打印使用说明 --password-file 设置包含身份验证密码的文件路径 -P 从控制台读取密码 --password \u003cpassword\u003e 设置认证密码 --username \u003cusername\u003e 设置认证用户名 --verbose 工作时打印更多信息 --connection-param-file \u003cfilename\u003e 提供连接参数的可选属性文件 --relaxed-isolation 将连接事务隔离设置为映射器读取未提交。 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:5:1","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r验证参数 参数 描述 --validate 启用复制数据的验证，仅支持单表复制。 --validator \u003cclass-name\u003e 指定要使用的验证器类。 --validation-threshold \u003cclass-name\u003e 指定要使用的验证阈值类。 --validation-failurehandler \u003cclass-name\u003e 指定要使用的验证失败处理程序类。 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:5:2","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r导出控制参数 参数 描述 --columns \u003ccol,col,col…\u003e 要导出到表的列 --direct 使用直接导出快速路径 --export-dir \u003cdir\u003e 导出的 HDFS 源路径 -m,--num-mappers \u003cn\u003e 使用_n个_map任务并行导出 --table \u003ctable-name\u003e 要填充的表格 --call \u003cstored-proc-name\u003e 要调用的存储过程 --update-key \u003ccol-name\u003e 用于更新的锚点列。如果有多个列，请使用逗号分隔的列表。 --update-mode \u003cmode\u003e 指定当数据库中发现具有不匹配键的新行时如何执行更新。 updateonly（默认）和 allowinsert。不存在会插入数据 --input-null-string \u003cnull-string\u003e 对于字符串列，将被解释为空的字符串 --input-null-non-string \u003cnull-string\u003e 对于非字符串列，将被解释为空的字符串 --staging-table \u003cstaging-table-name\u003e 数据在插入目标表之前将暂存于其中的表。 --clear-staging-table 指示可以删除暂存表中的所有数据。 --batch 使用批处理模式执行底层语句。 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:5:3","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r输入解析参数 争论 描述 --input-enclosed-by \u003cchar\u003e 设置必填字段 --input-escaped-by \u003cchar\u003e 设置输入转义字符 --input-fields-terminated-by \u003cchar\u003e 设置输入字段分隔符 --input-lines-terminated-by \u003cchar\u003e 设置输入行尾字符 --input-optionally-enclosed-by \u003cchar\u003e 设置字段包围字符 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:5:4","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r输出行格式化参数 争论 描述 --enclosed-by \u003cchar\u003e 设置必填字段括字符 --escaped-by \u003cchar\u003e 设置转义字符 --fields-terminated-by \u003cchar\u003e 设置字段分隔符 --lines-terminated-by \u003cchar\u003e 设置行尾字符 --mysql-delimiters 使用 MySQL 的默认分隔符集：字段：, 行：\\n 转义符：\\ 可选封闭符：' --optionally-enclosed-by \u003cchar\u003e 设置字段包围字符 Sqoop 会自动生成代码来解析和解释包含要导出回数据库的数据的文件记录。如果这些文件是使用非默认分隔符创建的（逗号分隔的字段和换行符分隔的记录），则应再次指定相同的分隔符，以便 Sqoop 可以解析您的文件。 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:5:5","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r代码生成参数 争论 描述 --bindir \u003cdir\u003e 编译对象的输出目录 --class-name \u003cname\u003e 设置生成的类名。这将覆盖 --package-name。与 结合使用时 --jar-file，设置输入类。 --jar-file \u003cfile\u003e 禁用代码生成；使用指定的 jar --outdir \u003cdir\u003e 生成代码的输出目录 --package-name \u003cname\u003e 将自动生成的类放入此包中 --map-column-java \u003cm\u003e 覆盖配置列的从 SQL 类型到 Java 类型的默认映射。 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:5:6","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":null,"content":"\r使用例子\rHDFS导出到MySQL sqoop export \\ --connect jdbc:mysql://192.168.109.1:3306/userdb \\ --username root \\ --password root \\ --table employee \\ --export-dir /emp/emp_data 更新导出只更新操作 sqoop export \\ --connect jdbc:mysql://192.168.109.1:3306/userdb \\ --username root --password root \\ --table updateonly \\ --export-dir /updateonly_2/ \\ # 更新导出 --update-key id \\ --update-mode updateonly # 只更新操作 更新插入模式 sqoop export \\ --connect jdbc:mysql://192.168.8.108:3306/ipv6_locate?serverTimezone=Asia/Shanghai\\\u0026useUnicode=true\\\u0026characterEncoding=utf8\\\u0026autoReconnect=true\\\u0026failOverReadOnly=false \\ # 有时候可能会出现中文乱码问题，需要添加编码集 --username root \\ --password passwd01 \\ --table mall_rpt_ip_with_scene \\ # 目标mysql中的表名 --update-key \"id\" \\ # 指定目标表的主键 --update-mode allowinsert \\ # 指定目标表的更新模式 --input-null-string '\\\\N' \\ --input-null-non-string '\\\\N' \\ --input-fields-terminated-by '\\001' \\ # hive中字段间的分隔符 --input-lines-terminated-by \"\\n\" \\ # hive中行与行之间的分隔符 --export-dir hdfs://cdh1.host.com:8020/user/hive/warehouse/wsp_test.db/mall_rpt_ip_with_scene \\ # hive表存放的位置 --m 1 # 并行度 ","date":"2024-06-06","objectID":"/bigdata/hadoop/20240606150541/:5:7","tags":["sqoop"],"title":"sqoop使用文档","uri":"/bigdata/hadoop/20240606150541/"},{"categories":["大数据"],"content":"Kerberos 是一种身份认证协议，被广泛运用在大数据生态中，甚至可以说是大数据身份认证的事实标准。本文将详细说明 Kerberos 原理。 ","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:0:0","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":["大数据"],"content":"\r二、Kerberos 是什么？Kerberos 一词来源于古希腊神话中的 Cerberus —— 守护地狱之门的三头犬。 一句话来说，Kerberos是一种基于加密Ticket的身份认证协议。 主要有三部分组成： Key Distribution Center (即KDC) Client Service ![[asserts/Pasted image 20240511160914.png]]客户端会先访问两次KDC，然后再访问目标Service，如：HTTP服务。 ","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:1:0","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":["大数据"],"content":"\r三、Kerberos 基本概念","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:2:0","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":["大数据"],"content":"\r3.1 基本概念 Principal：大致可以认为是 Kerberos 世界的用户名，用于标识身份。principal 主要由三部分构成：primary，instance(可选) 和 realm。包含 instance 的principal，一般会作为server端的principal，如：NameNode，HiverServer2，Presto Coordinator等；不含有 instance 的principal，一般会作为 客户端的principal，用于身份认证。例子如下图所示： Keytab：“密码本”。包含了多个 principal 与密码的文件，用户可以利用该文件进行身份认证。 Ticket Cache：客户端与 KDC 交互完成后，包含身份认证信息的文件，短期有效，需要不断renew。 Realm：Kerberos 系统中的一个namespace。不同 Kerberos 环境，可以通过 realm 进行区分。 ","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:2:1","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":["大数据"],"content":"\r3.2 KDCKey Distribution Center（即 KDC）, 是 Kerberos 的核心组件，主要由三个部分组成： Kerberos Database: 包含了一个 Realm 中所有的 principal、密码与其他信息。（默认：Berkeley DB） Authentication Service(AS): 进行用户信息认证，为客户端提供 Ticket Granting Tickets(TGT)。 Ticket Granting Service(TGS): 验证 TGT 与 Authenticator，为客户端提供 Service Tickets。 ","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:2:2","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":["大数据"],"content":"\r四、Kerberos 原理在深入了解 Kerberos 原理之前，先介绍一下 Kerberos 协议的几个大前提，帮助大家理解： Kerberos 基于 Ticket 实现身份认证，而非密码。如果客户端无法利用本地密钥，解密出 KDC 返回的加密Ticket，认证将无法通过。 客户端将依次与 Authentication Service, Ticket Granting Service 以及目标Service进行交互，共三次交互。 客户端与其他组件交互是，都将获取到两条信息，其中一条可以通过本地密钥解密出，另外一条将无法解密出。 客户端想要访问的目标服务，将不会直接与KDC交互，而是通过能否正确解密出客户端的请求来进行认证。 5. KDC Database 包含有所有 principal 对应的密码。 6. Kerberos 中信息加密方式一般是对称加密（可配置成非对称加密）。 下面，我们将以客户端访问 http 服务为例，解释整个认证过程。 ","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:3:0","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":["大数据"],"content":"\r4.1 客户端与 Authentication Service第一步，客户端通过kinit USERNAME或其他方式，将客户端ID, 目标HTTP服务ID, 网络地址（可能是多个机器的IP地址列表，如果想在任何机器上使用，则可能为空），以及TGT有效期的寿命等信息发送给 Authentication Service。 第二步，Authentication Server 将检查客户端ID是否在KDC数据库中。 如果 Authentication Server 检查操作没有异常，那么KDC将随机生成一个 key，用于客户端与 Ticket Granting Service(TGS) 通信。这个Key，一般被称为 TGS Session Key。随后 Authentication Server 将发送两条信息给客户端。示意图如下： 其中一条信息被称为TGT，由TGS的密钥加密，客户端无法解密，包含客户端ID, TGS Session Key等信息。另一条信息由客户端密钥加密，客户端可以正常解密，包含目标 HTTP 服务ID，TGS Session Key等信息。 第三步，客户端利用本地的密钥解密出第二条信息。如果本地密钥无法解密出信息，那么认证失败。示意图如下： ","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:3:1","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":["大数据"],"content":"\r4.2 客户端与 Ticket Granting Service这时候，客户端有了 TGT（由于本地没有TGS的密钥，导致无法解密出其数据）与 TGS Session Key。 第四步，客户端将: “无脑”将 AS 发送过来的TGT（由TGS密钥加密）转发给TGS 将包含自身信息的Authenticator(由TGS Session Key加密)发送给TGS 第五步，TGS 将利用 自身的密钥从TGT中解密出TGS Session Key，然后利用TGS Session Key从Authenticator 中解密出客户端的信息。 TGS 解密出所有信息后，将进行身份检查，进行认证： 将客户端ID与TGT的客户端ID进行比较 比较来自 Authenticator 的时间戳和TGT的时间戳 (典型的Kerberos系统的容忍度是2分钟，但也可以另行配置) 检查TGT是否过期 检查Authenticator是否已经在TGS的缓存中（为了避免重放攻击） 当所有检查都通过后， TGS 随机生成一个 Key 用于后续客户端与 HTTP 服务交互时进行通信加密使用，即 HTTP Session Key。同样地，TGS 将发送两条信息给客户端: 其中一条是 HTTP Ticket，由 HTTP 服务的密钥进行加密；另一条则由TGS Session Key加密，包含了客户端信息与时间戳。 第六步，客户端将利用TGS Session Key解密出其中一条信息，另一条信息由于是由目标HTTP服务加密，无法解密。 ","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:3:2","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":["大数据"],"content":"\r4.3 客户端与 HTTP Service这时候，客户端有了HTTP Ticket（由于本地没有HTTP服务的密钥，导致无法解密出其数据）与 HTTP Session Key。 第七步，客户端将: “无脑”将 AS 发送过来的 HTTP Ticket（由HTTP 密钥加密）转发给目标 http 服务。 将包含自身信息的Authenticator(由HTTP Session Key加密)发送给 http 服务。 第八步，HTTP服务首先利用自身的密钥解密出 HTTP Ticket 的信息，得到 HTTP Session Key；随后，利用HTTP Session Key解密出用户的Authenticator信息。 信息解密完成后，HTTP 服务同样需要做一些信息检查： 将 Authenticator 中的客户端ID与HTTP Ticket中的客户端ID进行比较 比较来自 Authenticator 的时间戳和 HTTP Ticket 的时间戳 (典型的 Kerberos 系统对差异的容忍度是 2 分钟，但也可以另行配置) 检查Ticket是否过期 检查 Authenticator 是否已经在HTTP服务器的缓存中（为了避免重播攻击） 至此，所有的认证过程通过，客户端即可与远程HTTP服务完成了身份认证，可以进行后续的信息通信。 ","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:3:3","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":["大数据"],"content":"\r五、Kerberos 的优势 密码无需进行网络传输。基于 Ticket 实现身份认证，保障密钥安全性。 双向认证。整个认证过程中，不仅需要客户端进行认证，待访问的服务也需要进行身份认证。 高性能。一旦Client获得用过访问某个Server的Ticket，该Server就能根据这个Ticket实现对Client的验证，而无须KDC的再次参与。 ","date":"2024-05-11","objectID":"/lang/docker/20240511160713/:4:0","tags":null,"title":"一文搞懂Kerberos","uri":"/lang/docker/20240511160713/"},{"categories":null,"content":"\rdocker命令官方文档docker | Docker Docs ","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:0:1","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"\r代理配置/etc/docker/daemon.json 文件写入下面内容(文件不存在新建) { \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ] } 重启docker服务 sudo systemctl daemon-reload sudo systemctl restart docker sudo service docker stop sudo service docker start ","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:0:2","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"\rdocker基础命令","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:1:0","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"\r增 docker pull 镜像文件名 #下载docker镜像 docker run hello-world #运行镜像文件,生成docker容器实例,docker run命令,会自动下载不存在的镜像 #容器是随时创建,随时删除的,轻量级,每次docker run 都会生成新的容器记录 #docker容器进程,如果没有在后台运行的话,就会立即挂掉, (容器中必须有正在工作的进程) #运行一个活着的容器进程 docker run -d centos /bin/sh -c \"while true;do echo '你个糟老头子,不听课,坏得很'; sleep 1;done\" #运行镜像 -d 后台运行的意思 centos 指的是镜像文件名 /bin/sh 要在这个容器内运行的命令,指定的解释器 shell解释器 -c 指定一段shell代码 #进入容器空间内的命令 docker exec -it 容器id /bin/bash #进入容器空间内 -i 交互式命令操作 -t 开启一个新的终端 exit 退出虚拟容器 #运行一个ubuntu容器 docker run -it ubuntu ","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:1:1","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"\r删 docker rmi 镜像名字/镜像id #删除镜像文件 docker rm 容器id/容器进程名字 #删除容器记录 docker rmi -f #强制删除镜像文件 docker rm `docker ps -aq` #批量删除容器记录,只能删除挂掉的容器记录 ","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:1:2","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"\r改 #docker容器进程的启停命令 docker start 容器id docker stop 容器id ","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:1:3","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"\r查 docker search 镜像文件名字 #搜索镜像文件 docker images #列出当前所有的镜像文件 docker ps #列出当前记录正在运行的容器进程 docker ps -a #列出所有的容器进程,以及挂掉的 docker logs 容器id #查看容器内的日志信息 docker logs -f 容器id #检测容器内的日志 ","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:1:4","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"\r容器镜像\r提交自己开发的dokcer镜像文件 docker run -it centos /bin/bash #进入一个纯净的centos容器空间内,此时是最小化安装的系统,没有vim 没有py3 在容器空间内 yum install vim ,然后退出容器 提交容器为新的镜像文件 # docker commit 容器进程id 镜像文件的名字 docker commit 419 s21docker-centos-vim 导出docker镜像 docker save 镜像文件名 \u003e /opt/s21-centos-vim.tar.gz 导入镜像文件 docker load \u003c 同事给你发的镜像文件 推送镜像只dockerhub # 仓库管理 创建dockerhub账号: # 修改镜像之后保存镜像: docker commit b573392cad1a mysql:5.7 # 修改镜像标签: docker tag mysql:5.7 bigox/mysql:5.7 # 登录docker: docker login # 推送镜像: docker push bigox/mysql:5.7 ","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:1:5","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"\r常用docker命令 docker容器信息 ##查看docker容器版本 docker version ##查看docker容器信息 docker info ##查看docker容器帮助 记不住docker命令可以经常使用docker --help查看 docker --help 镜像的操作 对镜像的操作，可使用镜像名、镜像长ID或短ID ### 镜像查看 docker image list ##只显示镜像ID docker images -q ##含中间映像层 docker images -qa ### 镜像搜索 docker search mysql # 镜像下载 docker pull redis # 下载仓库里的所有redis镜像 docker pull -a redis # 镜像删除 docker rmi redis docker rmi -f redis # 删除多个镜像 docker rmi -f redis tomcat nginx # 删除本地多个镜像 docker rmi -f $(docker images -q) # 构建镜像 ##（1）编写dockerfile cd /docker/dockerfile vim mycentos ##（2）构建docker镜像 docker build -f /docker/dockerfile/mycentos -t mycentos:1.1 ","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:1:6","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"\r容器操作 # 容器启动 ##新建并启动容器，参数：-i 以交互模式运行容器；-t 为容器重新分配一个伪输入终端；--name 为容器指定一个名称 docker run -i -t --name mycentos ##后台启动容器，参数：-d 已守护方式启动容器 docker run -d mycentos # 查看容器 ##查看正在运行的容器 docker ps ##查看正在运行的容器的ID docker ps -q ##查看正在运行+历史运行过的容器 docker ps -a ##显示运行容器总文件大小 docker ps -s ##显示最近创建容器 docker ps -l ##显示最近创建的3个容器 docker ps -n 3 ##不截断输出 docker ps --no-trunc # 容器进程 ##列出redis容器中运行进程 docker top redis ##查看所有运行容器的进程信息 for i in `docker ps |grep Up|awk '{print $1}'`;do echo \\ \u0026\u0026docker top $i; done # 容器日志 docker logs rabitmq ##查看redis容器日志，参数：-f 跟踪日志输出；-t 显示时间戳；--tail 仅列出最新N条容器日志； docker logs -f -t --tail=20 redis ##查看容器redis从2019年05月21日后的最新10条日志。 docker logs --since=\"2019-05-21\" --tail=10 redis # 容器的进入和退出 ##使用run方式在创建时进入 docker run -it centos /bin/bash ##关闭容器并退出 exit ##直接进入centos 容器启动命令的终端，不会启动新进程，多个attach连接共享容器屏幕，参数：--sig-proxy=false 确保CTRL-D或CTRL-C不会关闭容器 docker attach --sig-proxy=false centos ##在 centos 容器中打开新的交互模式终端，可以启动新进程，参数：-i 即使没有附加也保持STDIN 打开；-t 分配一个伪终端 docker exec -i -t centos /bin/bash ##以交互模式在容器中执行命令，结果返回到当前终端屏幕 docker exec -i -t centos ls -l /tmp ##以分离模式在容器中执行命令，程序后台运行，结果不会反馈到当前终端 docker exec -d centos touch cache.txt # 容器的停止和删除 ##停止一个运行中的容器 docker stop redis ##杀掉一个运行中的容器 docker kill redis ##删除一个已停止的容器 docker rm redis ##删除一个运行中的容器 docker rm -f redis ##删除多个容器 docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm ## -l 移除容器间的网络连接，连接名为 db docker rm -l db ## -v 删除容器，并删除容器挂载的数据卷 docker rm -v redis # 容器和主机间的数据拷贝 ##将rabbitmq容器中的文件copy至本地路径 docker cp rabbitmq:/[container_path] [local_path] ##将主机文件copy至rabbitmq容器 docker cp [local_path] rabbitmq:/[container_path]/ ##将主机文件copy至rabbitmq容器，目录重命名为[container_path]（注意与非重命名copy的区别） docker cp [local_path] rabbitmq:/[container_path] ","date":"2024-05-10","objectID":"/lang/docker/20240510103849/:1:7","tags":["docker"],"title":"docker使用常用命令","uri":"/lang/docker/20240510103849/"},{"categories":null,"content":"[TOC] ","date":"2024-05-08","objectID":"/db/20240508165000/:0:0","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"\r嵌入式SQL嵌入式SQL中通过主变量实现主语言和SQL语言间进行参数传递； SQL语句的执行状态通过sqlca，传递给主语言来进行流程控制； 对于返回结果为多条记录的sql语句，通过游标来由主语言逐条处理。 执行类型分析和检查：分析程序的类型系统，并检查程序是否违反类型规则(程序中变量和表达式的类型)， 编译的过程： 词法分析，将源代码分解为由单词组成的符号流的过程。 语法分析，语法分析，根据语法规则检查符号流的正确性，并生成语法树的过程 语义分析，检查程序的语义是否正确，并进行类型检查，是否符合语言规范 中间代码生成，根据语法树生成中间代码的过程，中间代码是一种抽象的机器代码与目标机器无关。 DFA能识别什么？ ![[asserts/Pasted image 20240515104903.png]] 主存主要采用动态随机存储器DRAM Cache采用静态随机存储器SRAM EEPROM是电擦除可编程的只读存储器 ![[asserts/Pasted image 20240515105946.png]] 入侵检测技术 海明码 系统页面大小为 ","date":"2024-05-08","objectID":"/db/20240508165000/:1:0","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"\r虚拟内存逻辑地址为 0001110100010110 逻辑地址内存分两部分：虚拟页号+页内偏移 虚拟页号：虚拟页号逻辑地址的最高位部分 页内偏移：逻辑地址的低位部分, 先根据页大小计算 页内地址长度需要多少位来表示 页大小为4K 4*2^10=2^12 低12位为页内偏移，页内地址 剩下的为页号 ![[asserts/Pasted image 20240522125000.png]] ","date":"2024-05-08","objectID":"/db/20240508165000/:1:1","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"\r进程管理","date":"2024-05-08","objectID":"/db/20240508165000/:1:2","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"\r数据结和算法 广义表的长度：广义表中原子或子表的数量， 广义表 {{a,b,c}} 中只有一个子表 {a,b,c}，因此它的长度为 1。 {a,{b,c,d}} 中，它包含一个原子和一个子表，因此该广义表的长度为 2 广义表的深度：观察表中所包含括号的层数间接得到 广义表 {{1,2},{3,{4,5}}} 中，子表 {1,2} 和 {3,{4,5}} 位于同层，此广义表中包含 3 层括号，因此深度为 3。 二叉树 堆： 通常是完全二叉树 最大堆：父节点的值比子节点要大 最小堆：父节点的值比子节点小 总结点 = 度为0 + 度为1 + 度为2(叶子节点-1) 叶子节点度为0 在二叉树中，总结点的数量等于度为0的结点数加度为1的结点数加度为2的结点数。已知二叉树中有70个叶子结点和80个度为1的结点，由于叶子结点的度数为0，度为2的结点数等于叶子结点数减1，即个。 排序![[asserts/Pasted image 20240522134301.png]] ","date":"2024-05-08","objectID":"/db/20240508165000/:1:3","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"\r数据库\r数据库设计![[asserts/Pasted image 20240522141345.png]] 模式：数据库的逻辑结构和数据组织方式 外模式：数据库系统的最外层，用户与数据库系统交互的接口，不同的用户或应用程序可以有不同的外模式。 内模式：数据库系统的最内层，描述了数据在物理介质上的存储结构和存取方法 数据在磁盘上的存储结构 存储记录的格式和排列 索引，数据压缩等技术 数据的独立性： 物理独立性：数据库的内模式发生改变时，数据的逻辑结构不变 数据独立性：外模式和模式之间的映射 数据的独立性是由DBMS的二级映像功能来保证的。数据的独立性包括数据的物理独立性和数据的逻辑独立性。数据的物理独立性是指当数据库的内模式发生改变时，数据的逻辑结构不变。为了保证应用程序能够正确执行，需要通过修改概念模式/内模式之间的映像。数据的逻辑独立性是指用户的应用程序与数据库的逻辑结构是相互独立的。数据的逻辑结构发生变化后，用户程序也可以不修改。但是，为了保证应用程序能够正确执行，需要修改外模式/概念模式之间的映像。 ","date":"2024-05-08","objectID":"/db/20240508165000/:1:4","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"\r关系泛式\r几种泛式 泛式 概念 判断方法 1NF 每一个属性都是不可分割的原子值 每一个字段仅包含一个值 每一列都是原子的，不能包含子表或数组 2NF 每一个非主属性完全依赖于主键(消除部分依赖) - 满足1NF - 非主属性完全依赖与整个主键，而不是主键的一部分 - 若存在部分依赖(某些非主属性依赖主键的一部分) 分解为更小的表 3NF 不存在非主属性对候选码的传递依赖 满足2NF 没有非主属性通过另一个非主属性，间接依赖主键 存在传递依赖，则需要将其分解我更小的表 BCNF 没有传递依赖和部分依赖， 每一个属性（或属性组合）若能决定其他属性，则该属性（或属性组合）是候选键 4NF 没有多值依赖 满足3NF 确保关系中没有多值依赖（即一个属性集的值集合与另一个属性集的值集合相互独立） 1NF：消除重复组，确保每个属性值是原子的。 2NF：在1NF的基础上，消除部分依赖，确保每个非主属性完全依赖于主键。 3NF：在2NF的基础上，消除传递依赖，确保每个非主属性直接依赖于主键。 BCNF：在3NF的基础上，消除非候选键的决定因素，确保每个决定因素都是候选键。 4NF：在BCNF的基础上，消除多值依赖，确保没有多值依赖。 关系代数 选择（Selection, σ）：从关系中选出满足特定条件的元组。 记号：σ条件(关系) 示例：σage \u003e 30(Employees) 投影（Projection, π）：从关系中选出特定的属性列，去除重复的元组。 记号：π属性列表(关系) 示例：πname, age(Employees) 并（Union, ∪）：合并两个关系，包含所有在两个关系中的元组，去除重复。 记号：R ∪ S 差（Difference, −）：获取属于第一个关系但不属于第二个关系的元组。 记号：R − S 笛卡尔积（Cartesian Product, ×）：将两个关系中的每一个元组合并形成一个新的元组。 记号：R × S 连接（Join, ⨝）：根据特定条件将两个关系中的元组合并形成一个新的元组。 记号：R ⨝条件 S 自然连接（Natural Join, ⨝）：在两个关系中查找相同属性名的列，并自动进行连接。 记号：R ⨝ S 除（Division, ÷）：在关系中查找那些在另一个关系中的所有元组都有匹配的元组。 记号：R ÷ S 关系元组演算常用的查询语言：域关系演算 元祖关系演算，关系代数 公式和符号的意思： 元组变量表示关系中的元组 如T ∈ R表示元组T属于关系R，T.A = T.B表示元组T的属性A等于属性B。 逻辑运算符 ∧(和)、∨或、¬ 非 绑定变量 在公式中被量词绑定的变量 比如，∃T (T ∈ R ∧ T.A = 5)中，T是绑定变量。 示例 # 查询年龄大于30的员工的名称和年龄 { T | T ∈ Employee ∧ T.Age \u003e 30 } **查询工资高于50000的员工的名字** { T.Name | ∃T (T ∈ Employee ∧ T.Salary \u003e 50000) } # 域关系演算 一个关系`Employee(EID, Name, Age, Salary)` -- 查询年龄大于30的员工的名称和年龄 { \u003cn, a\u003e | ∃e ∃s (Employee(e, n, a, s) ∧ a \u003e 30) } ","date":"2024-05-08","objectID":"/db/20240508165000/:1:5","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"\r分布式系统CAP分布式系统的基本原理代表三个关键属性：一致性（Consistency）、可用性（Availability）和分区容忍性（Partition Tolerance） 五个答题考点： 数据库概念结构设计 关系理论 泛式和主键候选键 函数依赖 三泛式 四泛式 泛式分解 SQL相关 事务的概念、锁死、并发 数据备份和故障恢复 数据库日志文件 ","date":"2024-05-08","objectID":"/db/20240508165000/:1:6","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"\r信息安全 主动攻击 攻击者干扰系统或网络的正常运行，通过修改、破坏或伪造数据，获取未授权的访问权限或恶意操作 拒绝服务攻击 DDOS DOS: 攻击者通过发送大量请求，消耗系统资源，使系统无法正常提供服务 中间人攻击MITM: sql注入 钓鱼攻击 被动攻击 不干扰系统的正常运行，隐蔽性，无破坏性，信息收集 流量分析，窃听，被动扫描 中国自主研发的3G通信标准是 TD-SCDMA 宏病毒一般感染 doc 为扩展名的文件 数字信封技术： 生成对称密钥 数据加密 对称密钥加密 传输加密数据和加密密钥 对称密钥解密 数据解密 计算机病毒： 文件型病毒 感染可执行文件 引导型病毒 影响软盘或硬盘的引导扇区 目录型病毒 修改硬盘上的所有文件的地址 宏病毒 使用某些程序创建出来的文本文档，数据库，电子表格等文件 防火墙的工作层次： 工作层次高，效率低，安全性高 受保护程度：从高到低 内网 DMZ 外网 ","date":"2024-05-08","objectID":"/db/20240508165000/:1:7","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"\r项目管理无主程序员组：适合开发小型项目 创新型项目， 确定性比较小的项目 ISO软件质量模型： 软件质量的6大特性：功能性、可靠性、易用性、效率、维护性、可移植性 软件质量的27个子特性： 功能性：适合性、准确性、互操作性、安全性、功能性的依从性 可靠性：成熟性、容错性、可恢复性、可靠性的依从性 易用性：易理解、易学习、易操作、吸引性、可使用性的依从性 效率：时间特性、资源特性、效率的依从性 维护性：易分析性、稳定性、易变更性、易测试性、可维护性的依从性 可移植性：适应性、易安装型、遵循性易替换性、可移植性的依从性 RUP：统一软件开发过程 角色、活动、制品和工作流4种重要的模型元素 统一过程模型： 关键特点：迭代和增量开发 基于用例的需求驱动 架构驱动 风险驱动 阶段 初始阶段 细化阶段 构建阶段 移交阶段 传统工程模型： 瀑布模型是最早出现的过程模型，它将软件生存周期的各项活动规定为按固定顺序而连接的若干阶段工作，形如瀑布流水，最终得到软件产品，主要的阶段包括需求分析、系统设计、详细设计、编码、单元测试、集成测试、系统测试、系统提交等。 v模型是对瀑布模型的一种改进模型，由于其模型构图形似字母v，所以又称为软件测试的V模型。V模型中，增加了测试环节的回溯验证，一般情况下，单元测试所对应的是详细设计环节；集成测试对应概要设计环节；系统测试对应需求分析环节；验收测试对应用户原始需求。 原型化模型:。原型化模型首先是建造一个快速原型，在充分了解和确认后，在原型基础上开发出用户满意的产品。 螺旋模型是一种演化软件开发过程模型，它兼顾了快速原型的迭代的特征以及瀑布模型的系统化与严格监控。螺旋模型最大的特点在于引入了其他模型不具备的风险分析，使软件在无法排除重大风险时有机会停止，以减小损失。同时，在每个迭代阶段构建原型是螺旋模型用以减小风险的途径。 增量模型：把软件产品作为一系列的增量构建来设计，编码，集成和测试，可以在增量开发过程中逐步理解需求 软件设计： 概要设计阶段 详细设计阶段 结构化设计方法： 概要设计阶段：软件体系结构的设计，数据设计和接口设计 详细设计阶段：数据结构和算法的设计 面相对象的设计方法： 概要设计阶段：体系结构设计，初步的类设计、数据设计，结构设计 详细设计阶段：构件设计 数据流图： 建模的时候应遵循自顶向下，从抽象到具体的建模思想。 描述数据的处理流程 底层数据流图描述了系统的输入输出 加工描述了数据流的转换 软件工程方法：进行软件开发，涉及到方法、工具和过程等要素。 项目风险：风险的优先级根据风险暴露 指标来表示，=风险影响(当风险发生时造成的损失) * 风险概率(风险发生的可能性) 项目风险：预算，进度，人员资源以及客户相关的问题 技术风险：设计，实现，对接，测试和维护未提 业务风险：建立一个无人想要的优秀产品风险，失去预算，人员承诺的风险 商业风险：市场风险，策略风险，管理风险和预算风险 UML图： 统一建模语言 用例图 描述系统的功能需求以及系统与外部用户之间的交互 类图 描述系统中的类及其属性、方法和类之间的关系 对象图：描述系统在某一时刻的对象实例及其关系 序列图：描述对象之间的交互顺序，强调时间顺序 活动图：类似于流程图 状态图：描述对象状态变化以及时间如何导致状态的转换 组件图：描述系统 的物理组件以及相互关系 部署图：描述系统在物理硬件上的部署结构 干货！14种uml图类型及示例 (boardmix.cn) ","date":"2024-05-08","objectID":"/db/20240508165000/:1:8","tags":null,"title":"软考知识点","uri":"/db/20240508165000/"},{"categories":null,"content":"0 行首 $ 行尾 i 当前游标插入 A 行尾追加 d2w 删除两个单词 dd 删除一行 2dd 删除两行 u撤销 ctrl+r 重新执行 put ce 删除单词并修改 cc 删除某行并修改 c$ 删除到行为并修改 c0 删除到行首并修改 %快速找成对的括号 :s/thee/the/g 将一行的thee替换为 the ","date":"2024-04-28","objectID":"/posts/20240428183407/:0:0","tags":null,"title":"Vim使用技巧大全","uri":"/posts/20240428183407/"},{"categories":null,"content":"\r2 Java核心基础 我想和自己说：学习是反复的过程，学习新知识的时候要仔仔细细阅读，错过一个关键词，可能对知识就会有偏差的理解。如果现在没有心情，收藏起来，后面用到的时候再仔细看。希望有帮助。 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:0:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.0 基本概念先了解","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r类 类是Java中的基本编程单元，用于描述对象的属性和行为。通过实例化类，可以创建对象 类是封装了数据和方法的结构 Java中，类概念非常核心和基础，用于组织和构建整个程序。 类名和文件名是一致的(后面讲class关键词的时候会细聊) ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:1","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r接口 接口是一种抽象的类型，它定义了一组方法的签名，但不提供方法的具体实现。 在后面写面向对象章节的时候，会对类和接口进行详解。 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:2","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r变量 在程序执行过程中，值可以在某个范围内发生改变的量。 变量要明确保存数据的数据类型 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:3","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r包 将相关的类和接口组织在一起，一个包下面可以创建很多类文件和接口文件。直接引用功能包，可能节省代码量。 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:4","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r常量 在程序执行过程中，值不发生改变的量。 ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:1:5","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.1 注释什么是注释，就是对程序进行解释和说明的文字 单行注释 //单行注释 多行注释 /* 多行注释 */ 文档注释 /** * 这是一个文档注释示例 * 它通常包含有关类、方法或字段的详细信息 */ public class MyClass { // 类的成员和方法 } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:2:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.2 关键字被Java语言赋予了特殊含义的单词，在idea中会高亮显示，学习完基础知识后，可以再回过头 看看，对每个关键字的用法要熟悉，不然就是没入门 关键字官方文档 https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html 用于定义数据类型的关键字 class 类的标识 class HelloWorld {}, class是Java程序的基本构建块，它包含了数据和方法(死记就行)。Java文件名必须和类名保持一致，为了编译时能够正确识别和定位类(规定，死记就行); 如果一个文件中包含多个类，只能有一个类声明为 public，并且文件名必须与public类一致 public class MyClass { // 主类 文件名必须是 MyClass.java，程序入口点所在的类，必须要用public修饰，为了执行程序的时候，能够被jvm虚拟机访问到 public static void main(String[] args) { // 入口点 } } class AnotherClass {// 非公共类，在同一个文件中可以有多个} class YetAnotherClass {// 非公共类，建议每个类都存储在独立的文件中，提高代码可读性，如果有多个相关的类，放在同一个包内} interface 接口的标识 interface MyInterface {} enum 枚举 enum Day { SUNDAY, MONDAY, TUESDAY } byte、short、int、long、float、double、char、boolean(八大基本数据类型) void 声明方法没有返回值 流程控制关键字 if else switch case default while do for break continue return 包的关键字 package：将相关的类和接口组织在一起，一个包下面可以创建很多类文件和接口文件，在Java代码中 声明包 package com.example.myapp 包名通常是反转的域名，看到包就知道是什么功能 import 导包，引入其他包中的类 import com.otherpackage.OtherClass 访问权限修饰符关键字 访问控制权限在Java中为了管理类，方法，变量等成员在其他类中的可见性和访问权限。有利于控制代码的封装性，安全性和可维护性 private 私有的 成员只对声明它的类可见，其他类无法访问 protected 受保护访问 同一包内的类和所有子类可见 public: 成员对所有类可见，其他类可以自由访问 扩展：default（package-private）默认访问级别，没有修饰符，成员对统一包内的类可见 类，函数，变量修饰符关键字 abstract用于声明抽象类、抽象方法。抽象类不能被实例化，通常包含抽象方法，子类需要实现这些抽象方法。 什么是抽象类和抽象方法 final 用于声明不可被继承的类 final class FinalClass{};声明不可被重写的方法 final void finalMethod() {};声明不可被修改的变量 final int constantValue = 10; static表名具有静态属性。 // 静态字段属于类，所有类的实例共享相同的静态字段。 // 通过类名直接访问 class Myclass { static int staticField = 10; // 静态方法属于类，不用创建实例，通过类名直接访问 static void staticMethod() { } // 静态块 用于类加载时执行一些初始化操作 static {// 静态块} } synchronized 修饰方法或代码块，确保同一时刻只有同一个线程可以访问修饰的部分 类与类之间关系关键字 extends：用于类之间的继承关系 implements：接口的实现 实例相关关键字 new 创建对象实例 ClassName obj = new ClassName(); this：代表当前对象的引用，在类的方法中使用 this.variable指的是当前对象的成员变量 super：用于调用父类的方法或访问父类的成员 instanceof：用于检查对象是否是特定类的实例 if (obj instanceof ClassName) 异常处理关键字 try、catch、finally、throw、throws 其他修饰符关键字 native strictfp transient volatile assert：断言检查 assert age \u003e= 18 : \"必须年满18岁\"; ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:3:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.3 标识符标识符就是用来给类，接口，方法，变量，包等其名字的规则 类、接口 : 大驼峰命名法,第一个单词大写的是类\\接口 HelloWorld, VariableDemo 变量, 方法: 小驼峰命名法 第一个单词小写的是方法和变量 zhangSanAge, studentName 常量：所有字母都大写 包：所有字母全部都小写 com.baidu ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:4:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.4 数据类型Java是强类型语言，每一个数据都给出了明确的数据类型 基本数据类型(简称: 基本类型) byte, short, char, int, long, float, double, boolean 定义long类型的数据时, 数据后边要加字母L(大小写均可), 建议加L 定义float类型的数据时, 数据后边要加字母F(大小写均可), 建议加F 引用数据类型(简称: 引用类型) - String, 数组, 类, 接口 数据类型转换 不同类型的数据之间可能会进行运算，而这些数据取值范围不同，存储方式不同，直接进行运算可能会造成数据损失，所以需要类型转换 自动(隐式)类型转换 将取值范围小的类型自动提升为取值范围大的类型，byte、short、char--\u003eint--\u003elong--\u003efloat--\u003edouble 强制(显式)类型转换 将取值范围大的类型强制转换成取值范围小的类型. public static void main(String[] args) { double doubleValue = 123.456; int intValue = (int) doubleValue; // 强制将double转换为int System.out.println(\"原始double值：\" + doubleValue); System.out.println(\"强制转换后的int值：\" + intValue); /*原始double值：123.456 强制转换后的int值：123 */ } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:5:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.5 常量 整数常量 1 小数常量 3.14 字符常量 A B 20(20不是字符，是有两个字符组合成的) 字符串常量 \"abcd\" 布尔常量 true false 空常量 null ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:6:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.6 变量 在程序执行过程中，值可以在某个范围内发生改变的量叫变量 变量要有明确的数据类型 声明方式 数据类型 变量名=初始化的值; int a=10; 先声明，再赋值 int a; a=10; //1. 定义一个类, 类名叫: VariableDemo02 public class VariableDemo02 { //2. 定义main方法, 作为程序的主入口. public static void main(String[] args) { //3. 测试byte类型. //3.1 定义一个byte类型的变量, 变量名叫b, 初始化值为10. byte b = 10; //3.2 将变量b的值打印到控制台上. System.out.println(b); //4. 测试short类型. //4.1 定义一个short类型的变量, 变量名叫s, 初始化值为20. short s = 20; //4.2 将变量s的值打印到控制台上. System.out.println(s); //5. 测试char类型. //5.1 定义一个char类型的变量, 变量名叫c, 初始化值为'A'. char c = 'A'; //5.2 将变量c的值打印到控制台上. System.out.println(c); //6. 测试int类型 int a = 10; System.out.println(a); //7. 测试long类型, 数据后记得加字母L. long lon = 100L; System.out.println(lon); //8. 测试float类型, 数据后边加字母F. float f = 10.3F; System.out.println(f); //9. 测试double类型. double d = 5.21; System.out.println(d); //10. 测试boolean类型. boolean bb = true; System.out.println(bb); } } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:7:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.7 运算符","date":"2024-04-26","objectID":"/lang/java/20240426133928/:8:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.7.1分类 算术运算符 +, -, *, /, %, ++, – 变量前++ ：变量a自己加1，将加1后的结果赋值给b b=++a public static void main(String[] args) { int a = 1; int b = ++a; System.out.println(a);//计算结果是2 System.out.println(b);//计算结果是2 } 变量后++ ：变量a先把自己的值1，赋值给变量b public static void main(String[] args) { int a = 1; int b = a++; System.out.println(a);//计算结果是2 System.out.println(b);//计算结果是1 } 赋值运算符 =, +=, -=, *=, /=, %= 比较(关系)运算符 ==, !=, \u003e, \u003e=, \u003c, \u003c= 逻辑运算符 \u0026\u0026(并且), ||(或者), !(逻辑非), ^(逻辑异或) 异同的意思, 相同为false, 不同为true. public class LogicalOperatorsExample { public static void main(String[] args) { // 逻辑与运算符 (\u0026\u0026) boolean condition1 = true; boolean condition2 = false; // 如果两个条件都为true，则结果为true，否则为false boolean resultAnd = condition1 \u0026\u0026 condition2; System.out.println(\"逻辑与运算符 (\u0026\u0026) 结果：\" + resultAnd); // 逻辑或运算符 (||) // 如果至少一个条件为true，则结果为true，否则为false boolean resultOr = condition1 || condition2; System.out.println(\"逻辑或运算符 (||) 结果：\" + resultOr); // 逻辑非运算符 (!) // 如果条件为true，则结果为false；如果条件为false，则结果为true boolean resultNot = !condition1; System.out.println(\"逻辑非运算符 (!) 结果：\" + resultNot); } } 三元(三目)运算符 (关系表达式) ? 表达式1：表达式2； true执行1表达式，false执行2表达式 public class OperatorDemo04 { public static void main(String[] args) { //1. 定义两个int类型的变量a. b, 初始化值分别为10, 20 int a = 10, b = 20; //2. 通过三元运算符, 获取变量a和b的最大值. int max = a \u003c b ? b : a; //3. 将结果(最大值)打印到控制台上. System.out.println(max); } } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:8:1","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.8 流程控制","date":"2024-04-26","objectID":"/lang/java/20240426133928/:9:0","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.8.1 分支结构如果我们想某些代码是在满足条件的情况下, 才能被执行, 此时就需要用到选择结构了, 选择结构也叫分支结构, 主要分为以下两种: if语句, 主要用于范围的判断 switch.case语句, 主要用于固定值的判断. if分支 public class IfExample { public static void main(String[] args) { int num = 10; if (num \u003e 0) { System.out.println(\"数字是正数\"); } else if (num \u003c 0) { System.out.println(\"数字是负数\"); } else { System.out.println(\"数字是零\"); } } } switch分支 public class SwitchExample { public static void main(String[] args) { int dayOfWeek = 3; String dayName; switch (dayOfWeek) { case 1: dayName = \"星期一\"; break; case 2: dayName = \"星期二\"; break; case 3: dayName = \"星期三\"; break; case 4: dayName = \"星期四\"; break; case 5: dayName = \"星期五\"; break; case 6: dayName = \"星期六\"; break; case 7: dayName = \"星期日\"; break; default: dayName = \"无效的日期\"; break; } System.out.println(\"今天是：\" + dayName); } } switch 分支 case穿透，在switch语句中，如果case的后面不写break，将出现case穿透现象，也就是不会在判断下一个case的值，直接向后运行，直到遇到break，或者整体switch结束。 import java.util.Scanner; public class SwitchDemo08 { public static void main(String[] args) { Scanner sc = new Scanner(System.in); System.out.println(\"请录入一个月份: \"); int month = sc.nextInt(); switch (month) { case 12: case 1: case 2: System.out.println(\"冬季\"); break; case 3: case 4: case 5: System.out.println(\"春季\"); break; case 6: case 7: case 8: System.out.println(\"夏季\"); break; case 9: case 10: case 11: System.out.println(\"秋季\"); break; default: System.out.println(\"没有这样的日期\"); break; } } } } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:9:1","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":"\r2.8.2 循环结构for循环 for(初始化条件1; 判断条件2; 控制条件3) { //循环体4; } for循环求1-100之前偶数和 public class ForDemo04 { public static void main(String[] args) { //1. 定义变量sum, 用来记录数据和. int sum = 0; //2. 通过for循环, 依次获取到1 - 100之间的数字. for (int i = 1; i \u003c= 100; i++) { //3. 判断当前遍历到的数字是否是偶数. if (i % 2 == 0) { //4. 走到这里, 说明是偶数, 累加给变量sum. sum += i; } } //5. 打印结果. System.out.println(\"1 - 100之间的偶数之和是: \" + sum); } } while循环 初始化条件1; while(判断条件2) { //循环体3; //控制条件4; } 例子 public class WhileDemo01 { public static void main(String[] args) { int i = 0; while(i \u003c 10) { System.out.println(\"Hello World!\"); i++; } } } 死循环：永远不结束的循环，循环的判断条件是true for(;;) { } while(true){} do {} while(true) 循环跳转: break continue public class ContinueExample { public static void main(String[] args) { for (int i = 1; i \u003c= 5; i++) { // 如果i为偶数，跳过本次循环 if (i % 2 == 0) { continue; } /*if (i==3) { break; }*/ System.out.println(\"当前数字是：\" + i); } } } 嵌套循环打印99乘法表 public class ForForDemo08 { public static void main(String[] args) { for (int i = 1; i \u003c= 9; i++) { //外循环控制行 for (int j = 1; j \u003c= i; j++) { //内循环控制列 //1 * 3 = 3 2 * 3 = 6 3 * 3 = 9 System.out.print(j + \" * \" + i + \" = \" + i * j + \"\\t\"); } System.out.println(); //内循环执行结束, 意味着一行打印完毕, 记得要换行. } } } ","date":"2024-04-26","objectID":"/lang/java/20240426133928/:9:2","tags":null,"title":"Java核心基础","uri":"/lang/java/20240426133928/"},{"categories":null,"content":" 前言：以初学着的身份，准备在该平台整理点最近学习的知识，方便后续查看相关的技术点，有兴趣的可以一块交流学习。目标用最少的文字，理解最多的知识。不说废话，字字珠玑。 ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:0:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.1 简介 强类型语言、开源、跨平台、多态、多线程、面向对象 完善的异常处理机制，大数据必备的语言 1995年出生，父亲 詹姆斯·高斯林(James Gosling)，2009年被Sun公司收购 兄弟版本 J2SE: 标准版, 也是其他两个版本的基础. 在JDK1.5的时候正式更名为: JavaSE. J2ME: 小型版, 一般用来研发嵌入式程序. 已经被Android替代了. 在JDK1.5的时候正式更名为: JavaME. J2EE: 企业版, 一般开发企业级互联网程序. 在JDK1.5的时候正式更名为: JavaEE ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:1:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.2 Java环境搭建 JDK和JRE区别 JRE: Java运行时环境(Java Runtime Environment) 运行Java的环境 JDK: Java development kit Java开发工具包，包含开发工具和JRE JVM：Java虚拟机(Java Virtual Machine) 和，Java运行环境里要有JVM 目录解释 bin: 存放的是编译器和工具 db: 存数数据 include: 编译本地方法. jre: Java运行时文件 lib: 存放类库文件 src.zip: 存放源代码的 对比这图片看能不能理解每个文件啥意思，不能理解，好好看看上面的介绍 怎么安装和配置path，不同的系统自己动手搜搜吧 ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:2:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.3 程序的开发步骤Java这种静态语言的的开发步骤一般分为 编写：编写源代码，在后缀名为.java的源文件中编写，用idea开发工具 编译:把源代码，编译成计算机能看懂的文件. javacv执行生成.class文件 执行:让计算机运行指定的代码程序 java 运行 直接整idea开发工具，来个hello world的例子 ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:3:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.4 idea开发工具 idea 下载地址 https://www.jetbrains.com/idea/ 激活方式 小程序 码叔资源 上找个激活码激活，或淘宝上买一个激活码 下载好后一路next安装，建议安装路径放D盘，软件是真大 ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:4:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1.5用idea开发一个helloworld 新建一个空项目 效果如下 我这一个空文件夹，怎么和Java关联起来呢，要和JDK关联上，才能用写Java代码是吧 打开刚刚新建的项目 有个 项目结构这个东西后面用到的挺多，配置 重点讲讲 project modules libraries的区别 项目：可以执行你刚刚创建那个空项目的名称，SDK(用Java还是什么开发),编译器输出路径 模块：我们新建一个day01的模块，后面可以在这个项目下新增day02 day03的模块，你也可以每天建一个Java项目。 建完之后项目下新增了一个day01的模块，可以在该模块下写代码了(真不容易) 删除模块 删除模块只是这个项目中看不到该模块了，在文件夹中还是存在的 依赖库 ：我这个项目想用第三方的包，咋个办，在这导入就行了，例如连接MySQL的包 搞了这么久，idea上写代码试下吧,day01模块下 src目录下新建一个 Java源代码文件,输入下面的代码 public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World!\"); } } idea使用小tips，idea很强大，这代码太繁琐了不会写怎么办，idea会自动补全的 输入main 按tab键，main方法出来了，sout 按tab键 print语句出来了，更多idea的技巧有机会，单独出一篇文章 运行代码初体验,直接右键run(idea会先编译，然后执行)，终端中会打印出helloworld ","date":"2024-04-26","objectID":"/lang/java/20240426111354/:5:0","tags":null,"title":"Java开发工具和前言","uri":"/lang/java/20240426111354/"},{"categories":null,"content":"\r1 网站 小林coding (xiaolincoding.com)🥓 🍳刘沙河 (bigox.wiki) 🧇爱编程的大丙 (subingwen.cn) ","date":"2024-04-25","objectID":"/web/:1:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"\r2 常用官方API MySQL官方文档🍕 Hive官方文档🍔 官方API- Spark 2.4.8🍟 ","date":"2024-04-25","objectID":"/web/:2:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"\r3 工具类 在线Cron表达式生成器 (qqe2.com) RegExr: Learn, Build, \u0026 Test RegEx The-X 在线工具箱 Base64 解码 AES RAS 解码 加密 ","date":"2024-04-25","objectID":"/web/:3:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"\r4 影视 BTNULL 无名小站🍿 ","date":"2024-04-25","objectID":"/web/:4:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"\r5 图书 安娜的档案 (annas-archive.org)🧂 ","date":"2024-04-25","objectID":"/web/:5:0","tags":null,"title":"fred导航","uri":"/web/"},{"categories":null,"content":"Hive官方操作手册 ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:0:0","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r1 Hive特性 hive基于Hadoop，将结构化数据映射为一张数据库表，提供类SQL查询功能 Hive元数据存储,通常存储在关系形数据库中，Hive中的元数据包括(表的名称，列，分区，是否为外部表等属性，数据所在的路径) Hive支持MapReduce、Tez和Spark 三种计算引擎。 数据格式。数据格式用户定义，根据三个属性：列分隔符（通常为空格、”\\t”、”\\x001″）、行分隔符（”\\n”）以及读取文件数据的方法（Hive 中默认有三个文件格式 TextFile，SequenceFile 以及 RCFile） 数据更新。不支持对数据的更新，使用更新可以借助Impala引擎+kudu数据格式 索引。有hive版本支持建立索引，但是各种问题，不建议使用。 Hive中包含的数据模型，DB、Table，External Table，Partition，Bucket db：在hdfs中表现为hive.metastore.warehouse.dir目录下一个文件夹。 table：在hdfs中表现所属db目录下一个文件夹。 external table：与table类似，不过其数据存放位置可以在任意指定路径。 partition：在hdfs中表现为table目录下的子目录。 bucket：在hdfs中表现为同一个表目录下根据hash散列之后的多个文件。 ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:1:0","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2 Hive表类型","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:0","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.1 hive数据类型 Hive的基本数据类型有：TINYINT，SAMLLINT，INT，BIGINT，BOOLEAN，FLOAT，DOUBLE，STRING，TIMESTAMP(V0.8.0+)和BINARY(V0.8.0+)。 Hive的集合类型有：STRUCT，MAP和ARRAY。 Hive表：内部表、外部表、分区表和桶表。 表的元数据保存传统的数据库的表中，当前hive只支持Derby和MySQL数据库。 ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:1","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.2 Hive内部表Hive中的内部表和传统数据库中的表在概念上是类似的，Hive的每个表都有自己的存储目录，除了外部表外，所有的表数据都存放在配置在hive-site.xml文件的${hive.metastore.warehouse.dir}/table_name目录下。 -- 创建内部表 CREATE TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, grade STRING COMMOT '班级'）COMMONT '学生表' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORE AS TEXTFILE; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:2","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.3 Hive外部表被external修饰的为外部表（external table），外部表指向已经存在在Hadoop HDFS上的数据，除了在删除外部表时只删除元数据而不会删除表数据外，其他和内部表很像 -- 创建外部表 CREATE EXTERNAL TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, class STRING COMMOT '班级'）COMMONT '学生表' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORE AS SEQUENCEFILE LOCATION '/usr/test/data/students.txt'; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:3","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.4 Hive分区表分区表的每一个分区都对应数据库中相应分区列的一个索引，分区表中的每一个分区对应一个目录文件，即同一个分区的数据存放一个目录下面，所以分区表如果很大，指定分区会速度快很多 比如说，分区表partitinTable有包含nation(国家)、ds(日期)和city(城市)3个分区，其中nation = china，ds = 20130506，city = Shanghai则对应HDFS上的目录为： /datawarehouse/partitinTable/nation=china/city=Shanghai/ds=20130506/。 -- 创建分区表 CREATE TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, class STRING COMMOT '班级'）COMMONT '学生表' PARTITIONED BY (ds STRING,country STRING) -- 分区的列不属于建表的字段中 ROW FORMAT DELIMITED --分隔符 FIELDS TERMINATED BY ',' -- 结束换行符 STORE AS SEQUENCEFILE -- 数据存储格式 ; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:4","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.5 Hive分桶表对指定列进行HASH，根据hash值进行切分数据，不同hash结果的数据写到每个桶对应的文件目录中 -- 创建分桶表 CREATE TABLE IF NOT EXISTS students(user_no INT,name STRING,sex STRING, class STRING COMMOT '班级',score SMALLINT COMMOT '总分'）COMMONT '学生表' PARTITIONED BY (ds STRING,country STRING) CLUSTERED BY(user_no) SORTED BY(score) INTO 32 BUCKETS -- 分桶 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORE AS SEQUENCEFILE; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:5","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.6 Hive视图逻辑数据结构，将查询结果作为视图，简化查询操作 CREATE VIEW employee_skills AS SELECT name, skills_score['DB'] AS DB, skills_score['Perl'] AS Perl, skills_score['Python'] AS Python, skills_score['Sales'] as Sales, skills_score['HR'] as HR FROM employee; ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:6","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":"\r2.7 总结表的创建官方标准 LanguageManual DDL -创建表的语法结构 CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables) -- 用另外一个表的结构来创建 CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; 删除表 DROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later) 清空表 TRUNCATE [TABLE] table_name [PARTITION partition_spec]; partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...) ","date":"2024-04-24","objectID":"/bigdata/hive/20240424162208/:2:7","tags":null,"title":"hive常见的命令","uri":"/bigdata/hive/20240424162208/"},{"categories":null,"content":" HDFS ","date":"2024-04-24","objectID":"/bigdata/hadoop/a9bc224/:0:0","tags":["Hadoop"],"title":"HDFS文件读写过程","uri":"/bigdata/hadoop/a9bc224/"},{"categories":null,"content":"\r1 文件的写入过程HDFS把文件的数据划分成若干个块(Block)， 每个Block存放在一组 DataNode上，Namenode负责维护文件–\u003eBlock(命令空间映射)和Block–\u003eDatanode(数据块映射) 写入流程 Client发起文件上传请求，通过RPC与NameNode建立通讯，NameNode检查目标文件是否已存在，父目录是否存在，返回是否可以上传 Client请求确定第一个block 应该传输到哪些Datanode服务器上 NameNode根据配置文件指定的备份数量以及机架感知原理进行文件分配，返回可用的 DataNode地址，如：A，B，C 备份数量：默认在HDFS上存放三份，本地一份、同机架内其他节点一份、不同机架节点一份 Client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline ），A 收到请求会继续调用 B，然后 B 调用 C，将整个 pipeline 建立完成， 后逐级返回 client； Client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存），以 packet 为单位（默认64K），A 收到一个 packet 就会传给 B，B 传给 C。A 每传一个 packet 会放入一个应答队列等待应答； 数据被分割成一个个 packet 数据包在 pipeline 上依次传输，在 pipeline 反方向上， 逐个发送 ack（命令正确应答），最终由 pipeline 中第一个 DataNode 节点 A 将 pipelineack 发送给 Client； 当一个 block 传输完成之后，Client 再次请求 NameNode 上传第二个 block，重复步骤 2； ","date":"2024-04-24","objectID":"/bigdata/hadoop/a9bc224/:1:0","tags":["Hadoop"],"title":"HDFS文件读写过程","uri":"/bigdata/hadoop/a9bc224/"},{"categories":null,"content":"\r2 HDFS文件读取过程 Client远程调用请求NameNode，获取文件块位置列表 NameNode会视情况返回文件的部分或者全部block列表；对于每个block，NameNode返回含副本的所有DataNode 地址；计算传输最快最优的DataNode 返回的DataNode地址，会按照集群拓扑结构计算客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 STALE，这样的排靠后； Client 选取排序靠前的 DataNode建立输入流，如果客户端本身就是DataNode，那么将从本地直接获取数据(短路读取特性)； 在选定的DataNode上读取该Block的数据 当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表； 读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的DataNode 继续读。 read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回Client请求包含块的DataNode地址，并不是返回请求块的数据； 最终读取来所有的 block 会合并成一个完整的最终文件。 总结：串行写入，数据包先发给节点A，然后节点A发送给B，B在给C 并行读取，并行读取block所在的节点，最后合并 ","date":"2024-04-24","objectID":"/bigdata/hadoop/a9bc224/:2:0","tags":["Hadoop"],"title":"HDFS文件读写过程","uri":"/bigdata/hadoop/a9bc224/"},{"categories":["project"],"content":"\r1 HDFS 的命令行使用Hadoop 命令行官方doc hadoop fs: 使用面最广，可以操作任务文件系统，包括本地文件系统、HDFS、FTP、S3等 hdfs dfs: 只能操作HDFS文件系统。 # 下面两种效果是一样的 hdfs dfs -ls /user/hive hadoop fs -ls /user/hive/ 如果操作HDFS文件系统，推荐使用 hdfs dfs,如果需要操作其他系统文件，使用hadoop fs命令 help 格式: hdfs dfs -help 操作命令 作用: 查看某一个操作命令的参数信息 ls 格式：hdfs dfs -ls URI 作用：类似于Linux的ls命令，显示文件列表 lsr 格式 : hdfs dfs -lsr URI 作用 : 在整个目录下递归执行ls, 与UNIX中的ls-R类似 mkdir 格式 ： hdfs dfs -mkdir [-p] \u003cpaths\u003e 作用 : 以\u003cpaths\u003e中的URI作为参数，创建目录。使用-p参数可以递归创建目录 put 格式 ： hdfs dfs -put \u003clocalsrc \u003e ... \u003cdst\u003e 作用 ： 将单个的源文件src或者多个源文件srcs从本地文件系统拷贝到目标文件系统中（\u003cdst\u003e对应的路径）。也可以从标准输入中读取输入，写入目标文件系统中 hdfs dfs -put /rooot/bigdata.txt /dir1 moveFromLocal 格式： hdfs dfs -moveFromLocal \u003clocalsrc\u003e \u003cdst\u003e 作用: 和put命令类似，但是源文件localsrc拷贝之后自身被删除 hdfs dfs -moveFromLocal /root/bigdata.txt / copyFromLocal 格式: hdfs dfs -copyFromLocal \u003clocalsrc\u003e ... \u003cdst\u003e 作用: 从本地文件系统中拷贝文件到hdfs路径去 appendToFile 格式: hdfs dfs -appendToFile \u003clocalsrc\u003e ... \u003cdst\u003e 作用: 追加一个或者多个文件到hdfs指定文件中.也可以从命令行读取输入. hdfs dfs -appendToFile a.xml b.xml /big.xml moveToLocal 在 hadoop 2.6.4 版本测试还未未实现此方法 格式：hadoop dfs -moveToLocal [-crc] \u003csrc\u003e \u003cdst\u003e 作用：将本地文件剪切到 HDFS get 格式 hdfs dfs -get [-ignorecrc ] [-crc] \u003csrc\u003e \u003clocaldst\u003e 作用：将文件拷贝到本地文件系统。 CRC 校验失败的文件通过-ignorecrc选项拷贝。 文件和CRC校验可以通过-CRC选项拷贝 hdfs dfs -get /bigdata.txt /export/servers getmerge 格式: hdfs dfs -getmerge \u003csrc\u003e \u003clocaldst\u003e 作用: 合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,... copyToLocal 格式: hdfs dfs -copyToLocal \u003csrc\u003e ... \u003clocaldst\u003e 作用: 从hdfs拷贝到本地 mv 格式 ： hdfs dfs -mv URI \u003cdest\u003e 作用： 将hdfs上的文件从原路径移动到目标路径（移动之后文件删除），该命令不能跨文件系统 hdfs dfs -mv /dir1/bigdata.txt /dir2 rm 格式： hdfs dfs -rm [-r] 【-skipTrash】 URI 【URI 。。。】 作用： 删除参数指定的文件，参数可以有多个。 此命令只删除文件和非空目录。 如果指定-skipTrash选项，那么在回收站可用的情况下，该选项将跳过回收站而直接删除文件； 否则，在回收站可用时，在HDFS Shell 中执行此命令，会将文件暂时放到回收站中。 hdfs dfs -rm -r /dir1 cp 格式: hdfs dfs -cp URI [URI ...] \u003cdest\u003e 作用： 将文件拷贝到目标路径中。如果\u003cdest\u003e 为目录的话，可以将多个文件拷贝到该目录下。 -f 选项将覆盖目标，如果它已经存在。 -p 选项将保留文件属性（时间戳、所有权、许可、ACL、XAttr）。 hdfs dfs -cp /dir1/a.txt /dir2/bigdata.txt cat hdfs dfs -cat URI [uri ...] 作用：将参数所指示的文件内容输出到stdout hdfs dfs -cat /bigdata.txt tail 格式: hdfs dfs -tail path 作用: 显示一个文件的末尾 text 格式:hdfs dfs -text path 作用: 以字符形式打印一个文件的内容 chmod 格式:hdfs dfs -chmod [-R] URI[URI ...] 作用：改变文件权限。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 hdfs dfs -chmod -R 777 /bigdata.txt chown 格式: hdfs dfs -chmod [-R] URI[URI ...] 作用： 改变文件的所属用户和用户组。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 hdfs dfs -chown -R hadoop:hadoop /bigdata.txt df 格式: hdfs dfs -df -h path 作用: 统计文件系统的可用空间信息 du 格式: hdfs dfs -du -s -h path 作用: 统计文件夹的大小信息 count 格式: hdfs dfs -count path 作用: 统计一个指定目录下的文件节点数量 setrep expunge (慎用) 格式: hdfs dfs -setrep num filePath 作用: 设置hdfs中文件的副本数量 注意: 即使设置的超过了datanode的数量,副本的数量也最多只能和datanode的数量是一致的 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:1:0","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":"\r2 hdfs的高级使用命令","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:0","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":"\r2.1 HDFS文件限额配置在多人共用HDFS的环境下，配置设置非常重要。特别是在 Hadoop 处理大量资料的环境，如果没有配额管理，很容易把所有的空间用完造成别人无法存取。HDFS 的配额设定是针对目录而不是针对账号，可以让每个账号仅操作某一个目录，然后对目录设置配置。 HDFS 文件的限额配置允许我们以文件个数，或者文件大小来限制我们在某个目录下上传的文件数量或者文件内容总量，以便达到我们类似百度网盘网盘等限制每个用户允许上传的最大的文件的量。 hdfs dfs -count -q -h /user/root/dir1 #查看配额信息 2.1.1 数量限额 hdfs dfs -mkdir -p /user/root/dir #创建hdfs文件夹 hdfs dfsadmin -setQuota 2 dir # 给该文件夹下面设置最多上传两个文件，发现只能上传一个文件 hdfs dfsadmin -clrQuota /user/root/dir # 清除文件数量限制 2.1.2 空间大小限额在设置空间配额时，设置的空间至少是 block_size * 3 大小 hdfs dfsadmin -setSpaceQuota 4k /user/root/dir # 限制空间大小4KB hdfs dfs -put /root/a.txt /user/root/dir # 生成任意大小的文件 dd if=/dev/zero of=1.txt bs=1M count=2 #生成2M的文件 # 清除空间配额限制 hdfs dfsadmin -clrSpaceQuota /user/root/dir ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:1","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":"\r2.2 HDFS 的安全模式安全模式是hadoop的一种保护机制，用于保证集群中的数据块的安全性。当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。 假设我们设置的副本数（即参数dfs.replication）是3，那么在datanode上就应该有3个副本存在，假设只存在2个副本，那么比例就是2/3=0.666。hdfs默认的副本率0.999。我们的副本率0.666明显小于0.999，因此系统会自动的复制副本到其他dataNode，使得副本率不小于0.999。如果系统中有5个副本，超过我们设定的3个副本，那么系统也会删除多于的2个副本。 在安全模式状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在，当整个系统达到安全标准时，HDFS自动离开安全模式。30s 安全模式操作命令 hdfs dfsadmin -safemode get #查看安全模式状态 hdfs dfsadmin -safemode enter #进入安全模式 hdfs dfsadmin -safemode leave #离开安全模式 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:2","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["开发"],"content":"\rHDFS HDFS概述 Hadoop 分布式系统框架中，首要的基础功能就是文件系统，在 Hadoop 中使用 FileSystem 这个抽象类来表示我们的文件系统，这个抽象类下面有很多子实现类，究竟使用哪一种，需要看我们具体的实现类，在我们实际工作中，用到的最多的就是HDFS(分布式文件系统)以及LocalFileSystem(本地文件系统)了。 在现代的企业环境中，单机容量往往无法存储大量数据，需要跨机器存储。统一管理分布在集群上的文件系统称为分布式文件系统。 HDFS（Hadoop Distributed File System）是 Hadoop 项目的一个子项目。是 Hadoop 的核心组件之一， Hadoop 非常适于存储大型数据 (比如 TB 和 PB)，其就是使用 HDFS 作为存储系统. HDFS 使用多台计算机存储文件，并且提供统一的访问接口，像是访问一个普通文件系统一样使用分布式文件系统。 HDFS架构 HDFS是一个主/从（Mater/Slave）体系结构，由三部分组成： NameNode 和 DataNode 以及 SecondaryNamenode： ● NameNode 负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。 ● DataNode 负责管理用户的文件数据块，每一个数据块都可以在多个 DataNode 上存储多个副本，默认为3个。 ● Secondary NameNode 用来监控 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS 元数据的快照。最主要作用是辅助 NameNode 管理元数据信息。 HDFS的特性 首先，它是一个文件系统，用于存储文件，通过统一的命名空间目录树来定位文件； 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 master/slave 架构（主从架构） HDFS 采用 master/slave 架构。一般一个 HDFS 集群是有一个 Namenode 和一定数目的 Datanode 组成。Namenode 是 HDFS 集群主节点，Datanode 是 HDFS 集群从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。 分块存储(Block) HDFS 中的文件在物理上是分块存储（block）的，块的大小可以通过配置参数来规定，默认大小在 hadoop2.x 版本中是 128M。 名字空间（NameSpace） HDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。 Namenode 负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被 Namenode 记录下来。 HDFS 会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。 NameNode 元数据管理 我们把目录结构及文件分块位置信息叫做元数据。NameNode 负责维护整个 HDFS 文件系统的目录树结构，以及每一个文件所对应的 block 块信息（block 的 id，及所在的 DataNode 服务器）。 DataNode 数据存储 文件的各个 block 的具体存储管理由 DataNode 节点承担。每一个 block 都可以在多个 DataNode 上。DataNode 需要定时向 NameNode 汇报自己持有的 block 信息。 存储多个副本（副本数量也可以通过参数设置 dfs.replication，默认是 3） 副本机制 为了容错，文件的所有 block 都会有副本。每个文件的 block 大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。 一次写入，多次读出 HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的修改。 正因为如此，HDFS 适合用来做大数据分析的底层存储服务，并不适合用来做网盘等应用，因为修改不方便，延迟大，网络开销大，成本太高。 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E6%A6%82%E8%A7%88/:0:0","tags":["Hadoop"],"title":"Hadoop概览","uri":"/bigdata/hadoop/hadoop%E6%A6%82%E8%A7%88/"},{"categories":["开发"],"content":"Configuration - Spark 2.4.8 配置参数 Spark SQL — PySpark官方API手册 ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:0:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":["开发"],"content":"\rApache Spark A Unified engine for large-scale data analytics Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing. ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:1:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":["开发"],"content":"\rDownloadingGet Spark from the downloads page of the project website. This documentation is for Spark version 3.5.1. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath. Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI. If you’d like to build Spark from source, visit Building Spark. ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:2:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"\rIntroductionThis is bold text, and this is emphasized text. Visit the Hugo website! ","date":"2024-04-20","objectID":"/posts/my-first-post/:1:0","tags":null,"title":"My First Post","uri":"/posts/my-first-post/"},{"categories":null,"content":"\r常用数据库的读写方式A blog (a truncation of “weblog”) is an informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (posts). Posts are typically displayed in reverse chronological order so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual,[citation needed] occasionally of a small group, and often covered a single subject or topic. In the 2010s, “multi-author blogs” (MABs) emerged, featuring the writing of multiple authors and sometimes professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other “microblogging” systems helps integrate MABs and single-author blogs into the news media. Blog can also be used as a verb, meaning to maintain or add content to a blog. ","date":"2024-02-20","objectID":"/posts/second_post/:1:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":null,"content":"\r标题二","date":"2024-02-20","objectID":"/posts/second_post/:2:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":null,"content":"\r标题三","date":"2024-02-20","objectID":"/posts/second_post/:3:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":["开发"],"content":"\rPythonpython第二篇文章扽扽 ","date":"2023-02-20","objectID":"/lang/python/first/:1:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":"\r标题二","date":"2023-02-20","objectID":"/lang/python/first/:2:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":"\r1 标题三","date":"2023-02-20","objectID":"/lang/python/first/:3:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":"\r1 Pythonpython第一篇文章扽扽 ","date":"2023-02-20","objectID":"/lang/python/second/:1:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":"\r2 标题二","date":"2023-02-20","objectID":"/lang/python/second/:2:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":"\r3 标题三","date":"2023-02-20","objectID":"/lang/python/second/:3:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":"\r常用数据库的读写方式A blog (a truncation of “weblog”) is an informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (posts). Posts are typically displayed in reverse chronological order so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual,[citation needed] occasionally of a small group, and often covered a single subject or topic. In the 2010s, “multi-author blogs” (MABs) emerged, featuring the writing of multiple authors and sometimes professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other “microblogging” systems helps integrate MABs and single-author blogs into the news media. Blog can also be used as a verb, meaning to maintain or add content to a blog. ","date":"2023-02-20","objectID":"/posts/first_post/:1:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"},{"categories":["开发"],"content":"\r标题二","date":"2023-02-20","objectID":"/posts/first_post/:2:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"},{"categories":["开发"],"content":"\r标题三","date":"2023-02-20","objectID":"/posts/first_post/:3:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"}]